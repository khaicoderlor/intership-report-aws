[{"uri":"https://khaicoderlor.github.io/intership-report-aws/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nSonrai đã giảm 70% thời gian nghiên cứu với AWS HealthOmics Tác giả: Jonah Craig, Kai Lawson-McDowall và Matthew Alderdice | Ngày 18 Tháng 7, 2025 | trong Amazon Athena , Amazon Bedrock , Amazon SageMaker Studio , Amazon Simple Storage Service (S3) , AWS HealthOmics , Chăm sóc sức khỏe , Ngành công nghiệp\nCác Quy trình làm việc Tin sinh học (Bioinformatics workflows), trải dài từ RNAseq đến Hệ chuyển hóa (metabolomics), đã trở nên ngày càng phức tạp và đòi hỏi nhiều dữ liệu (data-intensive). Đối với các nhà nghiên cứu trong ngành Công nghệ sinh học và Dược phẩm, việc trích xuất các thông tin chi tiết có ý nghĩa từ lượng lớn dữ liệu sinh học llà một thách thức hàng ngày. Nhận thấy điều này, Sonrai, một công ty dẫn đầu trong các công nghệ đám mây (cloud technologies) về Y học chính xác (precision medicine), đã bắt tay vào việc thay đổi bối cảnh phân tích Tin sinh học quy mô lớn (large-scale bioinformatics). Chúng ta sẽ thảo luận về hành trình của Sonrai trong việc khai thác Amazon Web Services (AWS) HealthOmics để phát triển một nền tảng hiệu quả, bảo mật và thân thiện với người dùng hơn nhằm lưu trữ các hệ thống xử lý Tin sinh học đa dạng (diverse bioinformatics pipelines). Sau khi triển khai HealthOmics, thời gian hoàn thành nghiên cứu đã được cải thiện 70%, trong khi chi phí thử nghiệm giảm mạnh tới 98,6%. Chúng ta sẽ khám phá cách giải pháp dựa trên đám mây này không chỉ tinh gọn các quy trình làm việc phức tạp mà còn trao quyền cho các nhà nghiên cứu điều hướng biển dữ liệu sinh học nhanh hơn và với chi phí thấp hơn.\nCơ hội: Đơn giản hóa y học chính xác Sonrai là một công ty tiên phong cung cấp các giải pháp phân tích nâng cao được thiết kế riêng cho nhu cầu của ngành công nghệ sinh học và dược phẩm. Với sản phẩm chủ lực của mình, Sonrai Discovery , công ty tạo điều kiện cho các nhà nghiên cứu phân tích và diễn giải dữ liệu phức tạp hơn, chẳng hạn như genomics (hệ gen), transcriptomics (hệ phiên mã) và quantitative proteomics (hệ protein định lượng). Điều này có vai trò then chốt trong việc hiểu về các căn bệnh và phát triển các phương pháp điều trị nhắm mục tiêu.\nĐối với nhiều phương thức, việc phân tích dữ liệu được tạo ra tại phòng thí nghiệm có thể đặt ra một số rào cản. Chẳng hạn, quy mô tuyệt đối của một số tập dữ liệu, như dữ liệu RNAseq đọc ngắn Illumina, thường có thể lên tới hàng trăm hoặc hàng nghìn gigabyte. Điều này đòi hỏi khả năng lưu trữ và xử lý có thể vượt quá khả năng của một máy trạm phòng thí nghiệm thông thường hoặc mất vài ngày để hoàn thành.\nKhi các phòng thí nghiệm tự phát triển các tập lệnh độc quyền của riêng họ để phân tích trình tự RNA, họ thường bỏ qua các bước kiểm soát chất lượng quan trọng, như loại bỏ bộ điều hợp (adapter removal), hoặc sử dụng các phiên bản phần mềm lỗi thời. Sự thiếu chuẩn hóa này dẫn đến một vấn đề nghiêm trọng: khả năng tái lập kết quả kém. Hậu quả có thể nghiêm trọng—các bài báo đã xuất bản có thể cần phải bị rút lại và các ứng viên thuốc đầy hứa hẹn có thể thất bại trong các thử nghiệm lâm sàng, cuối cùng gây lãng phí cả thời gian và vốn đầu tư.\nCác nghiên cứu đã chỉ ra rằng một tỷ lệ đáng kể các mẫu RNA-seq có thể thất bại trong kiểm soát chất lượng (QC) do quy trình phòng thí nghiệm không đầy đủ hoặc xử lý mẫu kém. Ví dụ, trong một nghiên cứu (in one study) sử dụng các mẫu FFPE (Formalin-Fixed Paraffin-Embedded), 40 mẫu đã thất bại ở bước thư viện tiền thu thập (pre-capture library step), và các mẫu bổ sung đã bị đánh dấu. Các mẫu gặp lỗi QC dựa trên các số liệu sinh học (như tương quan thấp theo mẫu trung vị (low median sample-wise correlation), số lượng đọc được ánh xạ (mapped reads) thấp, hoặc số lượng gen có thể phát hiện thấp).\nHơn nữa, việc các hệ thống xử lý nội bộ (in-house pipelines) thường xuyên phụ thuộc vào một hoặc nhiều cá nhân thường dẫn đến tài liệu kém, mã không thể di động và nợ kỹ thuật—khiến chúng khó hiểu, khó duy trì và khó cập nhật. Cuối cùng, các hệ thống xử lý nội bộ này hiếm khi xem xét bảo mật dữ liệu hoặc tuân thủ quy định dược phẩm , tạo ra một rủi ro pháp lý nghiêm trọng. Sonrai nhận thấy sự cần thiết của một giải pháp có thể mở rộng quy mô tính toán để xử lý các bộ dữ liệu lớn và cung cấp các quy trình làm việc (workflows) tiêu chuẩn nhưng có thể tùy chỉnh, được xây dựng dựa trên các nguyên tắc kỹ thuật phần mềm vững chắc. Họ cũng nhận thấy nhu cầu cung cấp một giao diện trực quan (intuitive interface) cho phép khách hàng xử lý dữ liệu một cách hiệu quả. Tiến sĩ Matthew Alderdice, Trưởng phòng Khoa học Dữ liệu tại Sonrai, cho biết: “Chúng tôi muốn các nhà nghiên cứu tập trung vào khoa học thay vì dành thời gian viết script và quản lý cơ sở hạ tầng kỹ thuật (technical infrastructure).”\nGiải pháp Sonrai:\nAWS HealthOmics thúc đẩy các đột phá khoa học ở quy mô lớn với quy trình làm việc sinh học được quản lý hoàn toàn. Nền tảng này cho phép lưu trữ các quy trình Tin sinh học nf-core (nf-core bioinformatics pipelines), là các quy trình làm việc Tin sinh học (bioinformatics workflows) được cộng đồng quản lý, chuẩn hóa cao, được xây dựng bằng hệ thống quản lý quy trình làm việc Nextflow. HealthOmics đã cho phép Sonrai lưu trữ các quy trình nf-core tùy chọn của họ, từ hơn 60 quy trình có sẵn. Điều này cho phép thực hiện một số lượng lớn các quy trình dữ liệu (chẳng hạn như toàn bộ hệ gen và hệ gen mục tiêu , proteomics khối phổ , và phiên mã học tế bào đơn và tế bào khối) mà không cần bất kỳ tập lệnh hoặc phát triển tùy chỉnh nào.\nHơn nữa, HealthOmics tự động điều chỉnh quy mô lưu trữ và yêu cầu tính toán (compute) khi chạy các hệ thống xử lý (pipelines) này. Điều này giúp khách hàng Sonrai không cần phải xác định trước yêu cầu tài nguyên. Nền tảng HealthOmics của Sonrai hoàn toàn dựa trên AWS, sử dụng một loạt các dịch vụ để cung cấp phân tích hiệu suất cao trong khi vẫn tuân thủ nghiêm ngặt các tiêu chuẩn ngành.\nHình 1 – Kiến trúc cấp cao Sonrai\nCác thành phần chính của giải pháp được cung cấp bởi AWS bao gồm:\nTriển khai cơ sở hạ tầng tự động: Sử dụng AWS Cloud Development Kit (AWS CDK), Sonrai xác minh việc triển khai tự động một môi trường an toàn, tuân thủ các quy định về dược phẩm—tuân thủ các thông lệ tốt nhất về bảo mật và quản trị dữ liệu bao gồm HIPAA, ISO và FedRAMP.\nLưu trữ dữ liệu tiết kiệm chi phí: Amazon Simple Storage Service ( Amazon S3 ) cung cấp các giải pháp lưu trữ có khả năng mở rộng và tiết kiệm chi phí, rất cần thiết để quản lý các tập dữ liệu có kích thước terabyte.\nCông cụ phân tích nâng cao: Sonrai sau đó tận dụng Amazon Athena để phân tích các tập dữ liệu này, một dịch vụ truy vấn tương tác giúp đơn giản hóa việc phân tích dữ liệu trên Amazon S3 bằng SQL chuẩn. Athena không cần máy chủ, vì vậy Sonrai không cần thiết lập hay quản lý cơ sở hạ tầng bổ sung mà chỉ cần trả tiền cho các tài nguyên được truy vấn. Sonrai sau đó tận dụng Amazon SageMaker Studio Lab , tích hợp liền mạch với AWS HealthOmics, cung cấp cho người dùng một giao diện hợp lý để truy cập các quy trình tin sinh học và phân tích dữ liệu.\nThông tin chi tiết dựa trên AI: Tận dụng Amazon Bedrock , nền tảng cung cấp nhiều lựa chọn mô hình nền tảng (FM) hiệu suất cao từ các công ty AI hàng đầu, Sonrai tự động hóa việc diễn giải kết quả quy trình. Điều này cho phép các nhà nghiên cứu nhận được thông tin chi tiết, được hỗ trợ bởi AI mà không cần xem xét thủ công.\nMang lại kết quả kinh doanh với AWS HealthOmics Bằng cách tích hợp các dịch vụ AWS, Sonrai đã đạt được một số kết quả kinh doanh quan trọng cho khách hàng của mình, bao gồm:\nGiảm chi phí: Khách hàng có thể tiết kiệm tới 98,6 phần trăm chi phí so với các nền tảng phân tích khác khi thực hiện quy trình.\nCải thiện tiến độ nghiên cứu: Việc triển khai các dự án R\u0026amp;D mới đã được rút ngắn từ vài tuần xuống còn vài ngày, với tiến độ nghiên cứu trung bình được rút ngắn 70 phần trăm.\nXử lý nhanh hơn: Với khả năng xử lý dữ liệu song song và không đồng bộ, AWS giảm đáng kể thời gian chạy phân tích dữ liệu, qua đó nâng cao hiệu quả vận hành.\nQuản lý cơ sở hạ tầng được tinh gọn: Bằng cách tự động hóa việc mở rộng tài nguyên và triển khai cơ sở hạ tầng, khách hàng Sonrai có thể tận dụng dịch vụ một cách liền mạch.\nTính nhất quán và tuân thủ: Quy trình chuẩn hóa và các biện pháp tuân thủ mạnh mẽ xác minh rằng mọi phân tích đều chính xác, có thể tái tạo và tuân thủ quy định dược phẩm.\nSonrai sẽ làm gì tiếp theo Sonrai cam kết tăng cường hợp tác với AWS, với những nỗ lực trong tương lai tập trung vào việc nâng cao Sonrai Discovery. Họ dự định tích hợp thêm các tính năng AWS HealthOmics (bao gồm kho lưu trữ trình tự sử dụng phân tầng thông minh Amazon S3 để tối ưu hóa chi phí lưu trữ). Việc tích hợp thêm nhiều tính năng sẽ tiếp tục giúp khách hàng của họ mở rộng ranh giới phân tích công nghệ sinh học và dược phẩm.\n“Chúng tôi rất hào hứng với sự hợp tác với AWS về HealthOmics. Tin sinh học là một lĩnh vực còn nhiều bất cập và thiếu tính nhất quán—điều này cho phép chúng tôi triển khai một lượng lớn quy trình làm việc theo cách cực kỳ nhất quán, tối ưu và đáng tin cậy cho khách hàng. Chúng tôi đã giảm thời gian triển khai mới cho mỗi quy trình từ vài tuần xuống còn chưa đầy một ngày”, Kai Lawson-McDowall, Chuyên gia Tin sinh học Cấp cao tại Sonrai, cho biết.\nNhờ sử dụng các dịch vụ AWS và các phương pháp tiếp cận sáng tạo, Sonrai đã dân chủ hóa tin sinh học bằng cách tạo ra các công cụ dễ tiếp cận cho người dùng thuộc mọi nền tảng kỹ thuật. Như Tiến sĩ Alderdice giải thích, “Với AWS HealthOmics, chúng tôi có thể cung cấp cho các công ty công nghệ sinh học và dược phẩm các phân tích tiên tiến để đẩy nhanh quá trình khám phá thuốc và rút ngắn đáng kể thời gian nghiên cứu tới 70%.”\nLiên hệ với Đại diện AWS để biết cách chúng tôi có thể giúp thúc đẩy doanh nghiệp của bạn.\nĐọc thêm Sonrai tăng tốc phân tích dữ liệu RNA-seq tế bào đơn bằng Amazon Bedrock (https://aws.amazon.com/solutions/case-studies/sonrai-bedrock-case-study/)\n150 Mô hình và Con số Đang Tiếp tục: Hướng dẫn của Bạn về Mô hình AI Tạo sinh cho Y tế và Khoa học Đời sống (https://aws.amazon.com/blogs/industries/150-models-and-counting-your-guide-to-generative-ai-models-for-healthcare-and-life-sciences/)\nKênh bài đăng trên blog AWS HealthOmics (https://aws.amazon.com/blogs/industries/category/artificial-intelligence/amazon-machine-learning/amazon-omics/)\n"},{"uri":"https://khaicoderlor.github.io/intership-report-aws/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://khaicoderlor.github.io/intership-report-aws/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://khaicoderlor.github.io/intership-report-aws/3-blogstranslated/3.4-blog4/","title":"Blog 4","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://khaicoderlor.github.io/intership-report-aws/3-blogstranslated/3.5-blog5/","title":"Blog 5","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://khaicoderlor.github.io/intership-report-aws/3-blogstranslated/3.6-blog6/","title":"Blog 6","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://khaicoderlor.github.io/intership-report-aws/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Summary Report: Vietnam Cloud Day 2025 – HCMC Connect Edition (Track 1: GenAI \u0026amp; Data) Event Objectives Understand security mindset: Learn methods to protect enterprises from emerging risks in the GenAI and AI Agents era Explore new development processes: Study the AI-Driven Development Lifecycle (AI-DLC) model to modernize workflows Optimize data foundation: Learn to build unified data architecture (Unified Data Foundation) for Analytics and AI Update strategy: Grasp the latest GenAI trends and technology roadmap from AWS Speakers Jun Kai Loke – AI/ML Specialist SA, AWS Kien Nguyen – Solutions Architect, AWS Tamelly Lim – Storage Specialist SA, AWS Binh Tran – Senior Solutions Architect, AWS Taiki Dang – Solutions Architect, AWS Michael Armentano – Principal WW GTM Specialist, AWS Key Highlights Building Unified Data Foundation (Unified Data Platform) Comprehensive data processing workflow (End-to-end): From ingestion, storage, processing to access and governance Solving the \u0026ldquo;Data Silo\u0026rdquo; problem: Breaking down barriers between departments and processes, moving toward Self-service model with strict Governance Core service ecosystem: Combining S3, Glue, Redshift, Lake Formation, OpenSearch, and Kinesis/MSK GenAI Deployment Strategy on AWS Vision direction and practical GenAI application roadmap Amazon Bedrock: Optimizing model selection, RAG deployment, and cost/latency control Expanding capabilities with AgentCore \u0026amp; Amazon Nova: Flexible integration with popular frameworks like CrewAI, LangGraph, LlamaIndex Multi-layered Security for GenAI Applications Identifying risks according to OWASP LLM standards and establishing security barriers from infrastructure, model to application layer 5 core security pillars: Compliance, Privacy, Controls, Risk Management, and Resilience Supporting tools: Bedrock Guardrails, Human-in-the-loop mechanism, and comprehensive monitoring (Observability) Accelerating Productivity with AI Agents Transitioning from simple virtual assistants to Multi-agent systems, minimizing manual intervention Case study: Applications in customer care and Business Intelligence (BI) through Amazon Q in QuickSight Ensuring Model Accuracy and Reliability Overcoming Hallucination phenomenon through Prompt Engineering, RAG, and Fine-tuning techniques RAG processing flow: Input → Embedding → Context Retrieval → LLM Processing → Output AI-Driven Development Lifecycle (AI-DLC) Development phases: Inception → Construction → Operation Evolution of AI\u0026rsquo;s role: From Support (AI-Assisted) → Direction (AI-Driven) → Management (AI-Managed) Implementation through: Infrastructure as Code (IaC), automated testing, and proactive risk management Amazon SageMaker Unified Studio Unified workspace for Data Engineering, Analytics, and AI Strong Lakehouse support and Zero-ETL data integration (between S3 and sources like Redshift, Aurora, DynamoDB) Comprehensive MLOps workflow: From pipelines, registry to deployment and monitoring Key Takeaways Systems Design Mindset Transitioning to End-to-end design thinking, focusing on eliminating bottlenecks (silos) in data flow Understanding the importance of establishing Governance rules alongside Self-service capabilities Technical Architecture Ability to seamlessly integrate AWS services (S3, Glue, SageMaker, Bedrock) to create a solid platform Recognizing the role of Zero-ETL and Lakehouse in ensuring scalability and sustainable operations How to use AI Agents and frameworks to automate complex tasks Deployment Strategy Need to build clear GenAI roadmap, balancing innovation and cost considerations Security is not an add-on feature but a foundation (infrastructure, model, app) combined with human element (Human-in-the-loop) AI output quality depends heavily on RAG techniques and Prompt Engineering Modern Software Development Mindset Shift in working model: AI is not just supporting but gradually participating in process management (AI-Managed) AI-DLC is the standardized framework for development teams to collaborate effectively with artificial intelligence Applying to Work For real-world projects: Pilot AI Agent to automate registration flow and support user inquiries Set up Guardrails to filter input/output content, ensuring GenAI application safety In team activities (Teamwork): Apply AI-DLC model: Use AI to create code base and documentation, team focuses on reviewing logic and making decisions Clearly analyze use-cases to choose between Serverless (Lambda) and Container (ECS/Fargate) to optimize costs Personal development direction: Practice Business-first mindset: Always put business problems first before choosing technology solutions Focus on strengthening Data Foundation knowledge as this is the \u0026ldquo;raw material\u0026rdquo; that determines GenAI success Event Experience Attending \u0026ldquo;Vietnam Cloud Day 2025\u0026rdquo; provided me with profound and practical insights into the current technology landscape, especially the intersection between Data and GenAI. Below are the highlights of my experience:\nExpert perspectives Listened to AWS experts deeply analyze Data Foundation and Security - the crucial icebergs beneath AI\u0026rsquo;s glamorous surface Better understanding of the Unified Data Platform concept, helping me visualize how a standard data system operates Strongly impressed by the vision of Multi-agent Systems, opening new directions for enterprise automation Hands-on experience Observed complete data pipeline process: From raw data ingestion to governance and utilization Direct exposure to advanced tools like Amazon Bedrock and SageMaker Unified Studio Witnessed the practical effectiveness of Hallucination handling techniques (RAG, Prompt Engineering) Understood how to divide work between Humans and AI through the AI-DLC model Exploring new technologies Recognized the necessity of Bedrock Guardrails in controlling AI content risks Clearly distinguished pros/cons between AWS Lambda and ECS/Fargate in specific scenarios Discovered the power of Amazon Q in supporting Business Intelligence (QuickSight) Networking and Lessons The event was a valuable opportunity to discuss with the Builders community, learning from both successful and failed case studies Biggest lesson: GenAI needs proper strategy and clean data foundation. Without Data and Security, AI is just an empty tool AI-DLC and Agents will be the future of software development, requiring programmers to adapt and upgrade AI management skills Overall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"},{"uri":"https://khaicoderlor.github.io/intership-report-aws/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Summary Report: AWS Cloud Mastery Series #1 – Generative AI, RAG \u0026amp; Agentic AI Event Objectives Optimize AI Interaction: Master Prompt Engineering techniques to maximize the power of large language models Leverage Existing Infrastructure: Explore AWS AI Services (Pre-trained) toolkit for rapid integration of intelligent features into applications Master Data: Deeply understand RAG (Retrieval-Augmented Generation) architecture to build AI that comprehends internal enterprise data Conquer Agentic AI: Grasp the trend shift from Chatbot to Agent and solutions to move Agents from proof-of-concept (POC) to production with Amazon Bedrock AgentCore Real-time Communication: Explore Pipecat Framework for developing real-time voice AI assistants Speakers Lâm Tuấn Kiệt – Sr DevOps Engineer (FPT Software) Danh Hoàng Hiếu Nghị – AI Engineer (Renova Cloud) Đinh Lê Hoàng Anh – Cloud Engineer Trainee (First Cloud AI Journey) Key Highlights Foundation Models Control Techniques Importance of effective communication with foundation models on Amazon Bedrock Zero-shot / Few-shot Prompting: Methods to guide responses through direct commands or specific examples Chain of Thought (CoT): Technique to stimulate models to think \u0026ldquo;step-by-step\u0026rdquo; for solving complex logic problems with higher accuracy AWS AI Services Ecosystem (Pre-trained) Using ready-to-use APIs (no model retraining needed) to solve specific problems:\nVision: Amazon Rekognition (image/video analysis) Language: Amazon Translate, Comprehend, Textract (text extraction) Audio: Amazon Polly (Text-to-Speech), Transcribe (Speech-to-Text) RAG Architecture (Retrieval Augmented Generation) Solution to minimize hallucination by providing real data context Amazon Titan Text Embeddings V2: Optimal tool for text vectorization and semantic search Knowledge Bases for Amazon Bedrock: End-to-end process from Chunking → Vector Storage → Retrieval → Generation The Era of Agentic AI and \u0026ldquo;Production\u0026rdquo; Challenges GenAI Evolution: From rule-following Assistants → Goal-oriented Agents → Autonomous Systems (multi-task autonomous systems) The Chasm: Major barriers when bringing Agents to market include Performance, Security, and state management Complexity Amazon Bedrock AgentCore Solution Platform helping developers overcome infrastructure barriers to focus on business logic.\nCore components:\nRuntime \u0026amp; Memory: Maintaining conversation context and learning history Identity \u0026amp; Gateway: Identity control and access security Code Interpreter: Ability to auto-generate and execute code for dynamic data processing Observability: Comprehensive Agent behavior monitoring Pipecat Framework – Real-time Voice AI Open-source framework optimized for Multimodal applications Processing Pipeline: WebRTC (Input) → STT → LLM Processing → TTS → Output (ultra-low latency) Key Takeaways Design mindset Shift to Action: Transitioning design thinking from passive \u0026ldquo;Q\u0026amp;A\u0026rdquo; to proactive \u0026ldquo;Task - Action\u0026rdquo; (Goal-oriented) Production-First: When building Agents, need to consider security, identity, and memory management from the start instead of just focusing on working logic Technical architecture Mastering data flow in RAG model: Embedding is the key for AI to understand text semantics Understanding Voice AI Pipeline structure: Seamless coordination between WebRTC, STT, LLM, and TTS to ensure real-time experience Development strategy Leverage Managed Services (like Bedrock AgentCore) to reduce operational burden (Undifferentiated heavy lifting), helping bring products to market faster Combine the power of Open Source (Pipecat, LangChain) with the stability of Cloud Services (AWS) Applying to Work Optimize workflow: Apply Chain of Thought (CoT) technique when writing prompts to enhance output quality for report analysis tasks Product development: Experiment with building an internal chatbot using Knowledge Bases for Amazon Bedrock to query company process documentation New technology research: Build a demo (POC) of an automated interview assistant using Pipecat Framework combined with Amazon Bedrock Skills roadmap: Focus on learning AgentCore configuration in depth, especially the Code Interpreter part for handling complex computational tasks Event Experience Attending the “GenAI-powered App-DB Modernization” workshop was extremely valuable, giving me a comprehensive view of modernizing applications and databases using advanced methods and tools. Key experiences included:\nLearning from highly skilled speakers Experts from AWS and major tech organizations shared best practices in modern application design. Through real-world case studies, I gained a deeper understanding of applying DDD and Event-Driven Architecture to large projects. Hands-on technical exposure Participating in event storming sessions helped me visualize how to model business processes into domain events. Learned how to split microservices and define bounded contexts to manage large-system complexity. Understood trade-offs between synchronous and asynchronous communication and integration patterns like pub/sub, point-to-point, streaming. Leveraging modern tools Explored Amazon Q Developer, an AI tool for SDLC support from planning to maintenance. Learned to automate code transformation and pilot serverless with AWS Lambda to improve productivity. Networking and discussions The workshop offered opportunities to exchange ideas with experts, peers, and business teams, enhancing the ubiquitous language between business and tech. Real-world examples reinforced the importance of the business-first approach rather than focusing solely on technology. Lessons learned Applying DDD and event-driven patterns reduces coupling while improving scalability and resilience. Modernization requires a phased approach with ROI measurement; rushing the process can be risky. AI tools like Amazon Q Developer can significantly boost productivity when integrated into the current workflow. Overall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"},{"uri":"https://khaicoderlor.github.io/intership-report-aws/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Nguyễn Quốc Khải\nPhone Number: 0978504380\nEmail: khainqse180154@fpt.edu.vn\nUniversity: FPT University\nMajor: Software Engineer\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 11/08/2025 to 09/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://khaicoderlor.github.io/intership-report-aws/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Understand cloud computing fundamentals and AWS global infrastructure Set up AWS account with proper security configurations Learn core AWS services: IAM, VPC, EC2 Introduction to version control with Git and Git Flow Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Mon Cloud \u0026amp; AWS Fundamentals: - Cloud computing concepts (IaaS, PaaS, SaaS) - AWS Global Infrastructure: Regions, AZs, Edge Locations - Create AWS Free Tier account Practice: Set up MFA, billing alerts 08/09/2025 08/09/2025 https://cloudjourney.awsstudygroup.com/ Tue IAM \u0026amp; Security Basics: - IAM users, groups, policies fundamentals - Root account protection best practices - Password policies and access keys Practice: Create IAM admin user, configure MFA for root 09/09/2025 09/09/2025 https://cloudjourney.awsstudygroup.com/ Wed Git \u0026amp; Version Control: - Git basics: init, add, commit, push, pull - Git Flow branching strategy (main, develop, feature, release, hotfix) - GitHub account setup Practice: Create first repository, practice branching 10/09/2025 10/09/2025 https://git-scm.com/ Thu VPC Networking Fundamentals: - VPC concepts: CIDR, subnets, route tables - Internet Gateway and NAT Gateway - Public vs Private subnets Practice: Create custom VPC with 2 public, 2 private subnets 11/09/2025 11/09/2025 https://cloudjourney.awsstudygroup.com/ Fri EC2 Introduction: - Instance types, pricing models (On-Demand, Reserved, Spot) - AMI and user data - Security Groups fundamentals Practice: Launch EC2 Linux, SSH connection, install basic tools 12/09/2025 12/09/2025 https://cloudjourney.awsstudygroup.com/ Week 1 Achievements: AWS Foundation:\nSuccessfully created AWS account with billing alerts and MFA enabled Understood AWS global infrastructure and region selection strategy Familiar with AWS Management Console navigation Security First:\nImplemented IAM best practices: admin user created, root account secured Configured password policy and MFA for all accounts Understood least privilege principle Version Control:\nSet up Git and GitHub account Learned Git Flow branching strategy for team collaboration Created first repository with proper README documentation Networking Skills:\nDesigned and deployed custom VPC with proper CIDR planning Understood difference between public and private subnets Configured Internet Gateway for public internet access Compute Basics:\nSuccessfully launched and connected to EC2 instance Understood different instance types and use cases Practiced SSH key management and Security Group configuration "},{"uri":"https://khaicoderlor.github.io/intership-report-aws/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Master EBS volumes and storage management Understand S3 object storage and security Learn AWS CLI for automation Introduction to CI/CD concepts and Jenkins basics Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Mon EBS Storage Deep Dive: - EBS volume types: gp3, io1/io2, st1, sc1 comparison - EBS snapshots and backup strategies - EBS encryption with KMS Practice: Create, attach, format EBS volumes, take snapshots 15/09/2025 15/09/2025 https://cloudjourney.awsstudygroup.com/ Tue S3 Fundamentals: - S3 bucket and object concepts - Storage classes: Standard, IA, Glacier, Intelligent-Tiering - Versioning and lifecycle policies Practice: Create buckets, upload objects, configure lifecycle rules 16/09/2025 16/09/2025 https://cloudjourney.awsstudygroup.com/ Wed S3 Security \u0026amp; Static Hosting: - Bucket policies and ACLs - S3 encryption (SSE-S3, SSE-KMS, SSE-C) - Static website hosting Practice: Configure bucket policies, host static website 17/09/2025 17/09/2025 https://cloudjourney.awsstudygroup.com/ Thu AWS CLI \u0026amp; Automation: - AWS CLI installation and configuration - Common CLI commands for EC2, S3, IAM - Shell scripting for AWS automation Practice: Automate resource creation with CLI scripts 18/09/2025 18/09/2025 https://cloudjourney.awsstudygroup.com/ Fri CI/CD Introduction \u0026amp; Jenkins: - CI/CD concepts and benefits - Jenkins architecture and installation - First pipeline: build, test, deploy Practice: Install Jenkins locally, create simple pipeline 19/09/2025 19/09/2025 https://jenkins.io/ Week 2 Achievements: Storage Mastery:\nSuccessfully configured all EBS volume types and understood performance characteristics Implemented automated snapshot backup strategy Practiced encryption at rest with AWS KMS S3 Expertise:\nCreated buckets with various storage classes Configured lifecycle policies to automatically transition objects Hosted first static website on S3 with custom error pages Security Implementation:\nConfigured granular bucket policies for least privilege access Implemented encryption for all S3 objects Understood difference between bucket ACLs and policies CLI Automation:\nInstalled and configured AWS CLI with access keys Created shell scripts to automate resource provisioning Practiced querying AWS services programmatically CI/CD Foundation:\nUnderstood continuous integration and deployment concepts Installed Jenkins and created first pipeline Learned about build automation and testing strategies "},{"uri":"https://khaicoderlor.github.io/intership-report-aws/1-worklog/","title":"Worklog","tags":[],"description":"","content":"This is a detailed worklog of the First Cloud Journey (FCJ) Workforce program completed over 12 weeks (84 days). Each week focuses on a different aspect of AWS Cloud, from fundamental foundations to advanced services, helping build solid knowledge for the AWS Cloud Engineer role.\n12-Week Learning Roadmap: Week 1: AWS Foundations - Getting Started with Account, IAM, VPC and EC2\nCreating AWS account, configuring IAM, understanding VPC and EC2 basics Week 2: Security and Networking - Security Foundations \u0026amp; Networking\nSetting up IAM Users/Groups, MFA, building Custom VPC with Public/Private Subnets Week 3: Computing and Application Identity - EC2 \u0026amp; Instance Profiling\nLaunching EC2, using IAM Roles, managing EBS Volumes Week 4: Storage and Database - Cloud9, S3 \u0026amp; RDS\nSetting up Cloud9 environment, deploying S3 Static Website, initializing RDS MySQL Week 5: Scalability - Lightsail \u0026amp; Auto Scaling\nGetting familiar with Docker, deploying Application Load Balancer and Auto Scaling Groups Week 6: Monitoring and Automation - CloudWatch \u0026amp; Lambda\nBuilding Dashboard, creating Alarms, writing Lambda functions for automation Week 7: Systems Management - AWS Systems Manager\nUsing SSM Session Manager, Run Command, managing fleet with Tagging Week 8: Infrastructure as Code - CloudFormation \u0026amp; CDK\nLearning CloudFormation, deploying infrastructure with AWS CDK (TypeScript) Week 9: Network and Cost Optimization - VPC Flow Logs \u0026amp; Cost Optimization\nAnalyzing network traffic, using Cost Explorer and Compute Optimizer Week 10: Serverless Architecture - API Gateway, Lambda \u0026amp; DynamoDB\nBuilding complete RESTful API with serverless architecture Week 11: Governance and Compliance - Governance \u0026amp; Well-Architected Review\nInfrastructure auditing, managing Service Quotas, applying Well-Architected Framework Week 12: Capstone Project and Career Readiness - Career Preparation\nDeploying comprehensive project, preparing for SAA-C03 certification, building portfolio "},{"uri":"https://khaicoderlor.github.io/intership-report-aws/5-workshop/5.1-workshop-overview/","title":"Workshop Overview","tags":[],"description":"","content":"Introduction to EV Rental AI Agent What is an AI Agent? An AI Agent is an intelligent system that can:\nUnderstand natural language queries Automatically select and execute appropriate tools/functions Make decisions based on context Provide structured responses with data Unlike traditional chatbots with fixed responses, AI Agents can reason and take actions dynamically.\nSystem Architecture Our EV Rental AI Agent uses a multi-layered architecture:\n┌─────────────────┐ │ User Interface │ ← React Frontend (Chat UI) └────────┬────────┘ │ HTTP/REST ↓ ┌─────────────────┐ │ FastAPI Server │ ← Backend Orchestrator └────────┬────────┘ │ ┌────┴────────────────┐ ↓ ↓ ┌──────────┐ ┌──────────────┐ │ Strands │ │ PostgreSQL │ │ Agent SDK│ │ (History) │ └────┬─────┘ └──────────────┘ │ ├─────→ AWS Bedrock (Claude 3.5 Sonnet) ├─────→ Knowledge Base (Policies/FAQ) └─────→ Backend API (Vehicles/Stations) Key Components Component Technology Purpose AI Model AWS Bedrock - Claude 3.5 Sonnet Natural language understanding \u0026amp; generation Agent Framework Strands Agent SDK Automatic tool selection \u0026amp; orchestration Backend API FastAPI (Python) REST API server for agent logic Database PostgreSQL Store chat history \u0026amp; sessions Frontend React + Chakra UI Interactive chat interface Knowledge Base AWS Bedrock KB Document retrieval (policies, FAQ) Core Features 1. Knowledge Base Search Agent searches through uploaded documents to answer questions about:\nRental policies Pricing information Booking procedures Terms and conditions Example Query:\n\u0026ldquo;Chính sách thuê xe của bạn là gì?\u0026rdquo;\nAgent Response:\n## 📋 VinFast Rental Policies ### 📄 Required Documents: - ✅ Valid ID/Passport - ✅ Driver\u0026#39;s License (Class B1+) - ✅ Proof of Residence ### 💰 Pricing: - **VF8**: 1,500,000 VNĐ/day - **VF9**: 2,000,000 VNĐ/day - **Deposit**: 10,000,000 VNĐ 2. Vehicle Search Agent queries the backend API to find available vehicles based on:\nLocation (city) Date range Vehicle model/type Response Format: Interactive vehicle cards with specs\n3. Charging Station Finder Agent retrieves nearby charging stations with:\nAddress and status Available chargers Distance (if location provided) Response Format: Station cards with real-time availability\nWorkshop Objectives By the end of this workshop, you will be able to:\n✅ Configure AWS Bedrock - Enable Claude models and create a Knowledge Base ✅ Build an AI Agent Backend - Use Strands SDK to orchestrate multiple tools ✅ Deploy a Chat Interface - Create a responsive React chat UI ✅ Test End-to-End - Interact with the AI agent and verify all functionalities Technology Stack AWS Services:\nAWS Bedrock (Claude 3.5 Sonnet v2) AWS Bedrock Knowledge Bases AWS S3 (for document storage) IAM (for access management) Backend:\nPython 3.11+ FastAPI Strands Agent SDK PostgreSQL SQLAlchemy Frontend:\nReact 18 Chakra UI Axios React Markdown Workshop Flow Step 1: Prerequisites ↓ Step 2: Setup AWS Bedrock \u0026amp; Knowledge Base ↓ Step 3: Deploy Backend API (FastAPI) ↓ Step 4: Deploy Frontend (React) ↓ Step 5: Test the AI Agent ↓ Step 6: Cleanup Resources Next: Let\u0026rsquo;s move to Prerequisites to prepare your environment.\n"},{"uri":"https://khaicoderlor.github.io/intership-report-aws/5-workshop/5.2-prerequiste/","title":"Prerequisites","tags":[],"description":"","content":"Prerequisites for EV Rental AI Agent Workshop Before starting this workshop, ensure you have the following requirements ready:\n1. AWS Account You need an AWS Account with appropriate permissions to:\nAccess AWS Bedrock service Create and manage IAM users Create S3 buckets (for Knowledge Base) Create Knowledge Bases Note: Bedrock is available in specific regions. Recommended regions:\nus-west-2 (Oregon) us-east-1 (N. Virginia) ap-southeast-1 (Singapore) 2. IAM User with Bedrock Permissions You need to create an IAM User with AWS Bedrock access for your application.\nStep 1: Create IAM User\nGo to AWS Console → IAM → Users → Create User User name: bedrock-agent-user ✅ Check: Provide user access to the AWS Management Console (optional) ✅ Select: I want to create an IAM user Click Next Step 2: Attach Permissions\nSelect: Attach policies directly Search and select these policies: ✅ AmazonBedrockFullAccess - Full access to Bedrock models and Knowledge Bases ✅ (Optional) AmazonS3ReadOnlyAccess - If using Knowledge Base with S3 Click Next → Create User Step 3: Create Access Keys\nClick on the newly created user: bedrock-agent-user Go to Security credentials tab Scroll down to Access keys → Click Create access key Select use case: Application running outside AWS Click Next → Create access key ⚠️ IMPORTANT: Copy and save: Access Key ID (example: AKIAIOSFODNN7EXAMPLE) Secret Access Key (shown only once, example: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY) Click Done ⚠️ Security Note:\n# Save to .env file (DO NOT commit to Git) AWS_ACCESS_KEY_ID=YOUR_ACCESS_KEY_ID_HERE AWS_SECRET_ACCESS_KEY=YOUR_SECRET_ACCESS_KEY_HERE AWS_REGION=us-west-2 3. Development Environment 3.1. Python Environment Python 3.11 or higher Package manager: pip Verify installation:\npython --version # Expected: Python 3.11.x or higher pip --version 3.2. Node.js Environment Node.js 18+ and npm Required for React frontend Verify installation:\nnode --version # Expected: v18.x.x or higher npm --version 3.3. PostgreSQL Database PostgreSQL 14+ installed locally or use Docker Option 1: Install locally\nDownload from: https://www.postgresql.org/download/ Create database: ev_rental_db Option 2: Use Docker\ndocker run -d \\ --name postgres-ev \\ -e POSTGRES_PASSWORD=password \\ -e POSTGRES_DB=ev_rental_db \\ -p 5432:5432 \\ postgres:14 Verify PostgreSQL:\n# Check PostgreSQL is running psql --version # Connect to database psql -U postgres -d ev_rental_db 4. Code Editor \u0026amp; Tools VS Code or your preferred IDE Git for cloning repositories Postman or cURL for API testing (optional) Install VS Code:\nDownload from: https://code.visualstudio.com/ Install Git:\n# macOS brew install git # Windows # Download from: https://git-scm.com/download/win # Verify git --version 5. AWS CLI (Optional) Install AWS CLI to interact with AWS services from command line:\n# macOS/Linux curl \u0026#34;https://awscli.amazonaws.com/AWSCLIV2.pkg\u0026#34; -o \u0026#34;AWSCLIV2.pkg\u0026#34; sudo installer -pkg AWSCLIV2.pkg -target / # Windows # Download from: https://awscli.amazonaws.com/AWSCLIV2.msi # Verify aws --version Configure AWS CLI:\naws configure # Enter your Access Key ID: AKIA5GPEMGJZK6E7PMEB # Enter your Secret Access Key: (paste your secret key) # Default region name: us-west-2 # Default output format: json Test AWS CLI:\n# List available Bedrock models aws bedrock list-foundation-models --region us-west-2 # Check your identity aws sts get-caller-identity Prerequisites Checklist Before proceeding to the next step, ensure you have:\n✅ AWS Account with Bedrock access in supported region ✅ IAM User created with AmazonBedrockFullAccess policy ✅ Access Key ID and Secret Access Key saved securely ✅ Python 3.11+ installed and verified ✅ Node.js 18+ and npm installed and verified ✅ PostgreSQL 14+ database running ✅ Code editor (VS Code recommended) installed ✅ Git installed and configured ✅ (Optional) AWS CLI installed and configured Estimated Costs This workshop uses the following AWS services:\nService Estimated Cost Notes AWS Bedrock - Claude 3.5 Sonnet ~$0.50 - $2.00 Pay per API call (input/output tokens) AWS Bedrock - Knowledge Base ~$0.10 - $0.50 Vector storage + retrieval S3 Storage ~$0.02 Minimal for documents Data Transfer ~$0.05 Usually within free tier Total ~$0.67 - $2.57 For the entire workshop 💡 Tip: Remember to clean up resources after the workshop to avoid ongoing charges!\nNext: Proceed to Setup AWS Bedrock to enable models and create Knowledge Base.\n"},{"uri":"https://khaicoderlor.github.io/intership-report-aws/2-proposal/","title":"Proposal","tags":[],"description":"","content":"The EV Station-based Rental System Electric Vehicle Rental and Return Software at Fixed Stations – A Green Mobility Solution for Smart Cities 📄 View Full Proposal Document (Google Docs)\n1. Executive Summary The EV Station-based Rental System is developed to provide an all-in-one platform for electric vehicle rental and charging management. It integrates real-time rental, payment, and charging station access through a unified cloud-native solution. The system features a React Native mobile app and a Spring Boot backend deployed on AWS ECS Fargate, with PostgreSQL (RDS) and Redis (ElastiCache) for data and caching. User authentication is managed via Amazon Cognito, and global delivery is optimized using CloudFront. Designed under the AWS Well-Architected Framework, the platform ensures scalability, high availability, and security while maintaining cost efficiency.\n2. Problem Statement What’s the Problem? Current electric vehicle (EV) rental services are fragmented, requiring users to switch between multiple apps to locate, book, and manage rentals at fixed points. This creates inconvenience, slow performance, and unreliable experiences — users often arrive at “unavailable” or “offline” rental points, leading to frustration and loss of trust.\nFor vehicle owners and operators, manual fleet management, booking coordination, and maintenance tracking result in operational inefficiencies and lost revenue. Currently, there is no unified, real-time platform connecting renters, vehicle owners, and rental point operators.\nThe Solution The EV Station-based Rental System consolidates EV rental and return at fixed points into a single, cloud-native platform. Built with React Native for mobile and Spring Boot for backend, the system delivers real-time booking, vehicle tracking, and payment integration.\nKey AWS services include ECS Fargate for compute, RDS PostgreSQL for data storage, ElastiCache for low-latency performance, API Gateway and Cognito for secure access, and CloudFront for global content delivery. The platform supports both fleet-based and peer-to-peer (P2P) vehicle registration, providing a centralized interface for users and operators to manage rentals efficiently, securely, and at scale.\nBenefits and Return on Investment The platform eliminates manual coordination and fragmented applications, offering a unified, automated experience for renters and fleet owners. Real-time data ensures reliability and transparency regarding vehicle availability and rental point status.\nDesigned under the AWS Well-Architected Framework, the system minimizes operational costs with a serverless, pay-per-use model while maintaining scalability and 99.99% uptime. Within 1 months, the platform is projected to reach 1,000+ monthly active users, onboard 200+ rental points, and deliver significant time, cost, and operational efficiencies for both users and operators.\n3. Solution Architecture The VoltGo platform is built on a secure, scalable, and fully managed AWS serverless architecture. Backend services are deployed on Amazon ECS Fargate with container images stored in Amazon ECR. The system uses Aurora PostgreSQL Serverless v2 for the main relational database and ElastiCache Serverless (Valkey/Redis) for low-latency caching and session management. All services operate within private subnets across multiple Availability Zones and are securely exposed through Amazon API Gateway integrated with Application Load Balancer via AWS PrivateLink. User authentication is handled by Amazon Cognito with JWT and MFA support. The frontend is hosted on Amazon S3 and delivered globally through Amazon CloudFront with AWS WAF and ACM SSL/TLS protection. AI-powered features are implemented using Amazon Bedrock integrated via AWS Lambda, while Amazon Location Service provides map and nearby station search functionality. Monitoring and logging are centralized in Amazon CloudWatch, and the entire infrastructure is provisioned using Terraform for automated and consistent deployments.\nAWS Services Used Amazon ECS Fargate: Serverless container orchestration for backend microservices. Amazon RDS PostgreSQL: Relational database. Amazon ElastiCache Serverless (Valkey): In-memory caching for low-latency data access. Amazon API Gateway: Secure REST API entry point integrated via PrivateLink. Amazon Cognito: User authentication and authorization with JWT and MFA. Amazon CloudFront + S3: Global content delivery and static hosting with WAF protection. Amazon CloudWatch: Unified monitoring, logging, and alerting for all services. Amazon ACM: Edge-level security and SSL/TLS certificate management. Amazon Application Load Balancer: Route traffic forward to target group Amazon Location Service: Providing map and places (index) for frontend allow user search and check station nearly. Amazon Bedrock: Handling chatting support user for QA and find nearby station base on user\u0026rsquo;s location Amazon EC2: Bastion Server connect RDS to config extension and running script Amazon Lambda: Bridge connect with Bedrock to handle api in QA from user Amazon ECR: Repository store image for ecs task Component Design Frontend:React web application hosted on Amazon S3 and delivered via CloudFront, secured ACM SSL/TLS certificates. API Layer: Amazon API Gateway provides the public API endpoint. Compute Layer: Amazon ECS Fargate runs containerized across multiple Availability Zones, scaling automatically based on CPU and memory utilization. Database Layer:Amazon PostgreSQL stores relational data for high availability and automated scaling. Caching Layer: Amazon ElastiCache Serverless (Redis) caches session and booking data to reduce database load and improve response time. Authentication: Amazon Cognito handles user registration, login, and JWT-based authorization with optional MFA support. Storage: Amazon S3 manages static assets and user uploads, accessible only through CloudFront via Origin Access Control (OAC). Monitoring \u0026amp; Security: Amazon CloudWatch tracks logs and performance metrics, while AWS Secrets Manager securely stores credentials with automatic rotation. 4. Technical Implementation Implementation Phases This project has two main parts—developing the backend locally and deploying it to the AWS cloud—each following four key phases:\n1.Build and Design Architecture: Develop and test backend services locally using Docker Compose, PostgreSQL, and Redis. Design the AWS serverless architecture including ECS Fargate, Aurora Serverless, ElastiCache, and API Gateway with PrivateLink connections. (Pre-deployment phase) 2.Estimate Cost and Validate Feasibility: Use AWS Pricing Calculator to estimate the monthly cost of ECS tasks, Aurora capacity units, and CloudFront bandwidth. Adjust design decisions to ensure cost-effectiveness and smooth migration. 3.Configure and Deploy Infrastructure: Build and deploy cloud infrastructure using Terraform for IaC. Configure VPC, ECS, Aurora, ElastiCache, Cognito, and CloudFront. Validate IAM roles, networking, and private-only access via VPC Endpoints. 4.Test, Optimize, and Release: Deploy Dockerized services to ECS Fargate, test API Gateway → PrivateLink → NLB → ECS flow, and verify database connections. Enable CloudWatch monitoring, auto-scaling, and WAF protection. Optimize scaling thresholds and document final architecture. Technical Requirements\nBackend Services: Node.js or Spring Boot microservices for Auth, Booking, and Payment, containerized with Docker and deployed to ECS Fargate (2–10 tasks, auto-scaling). Database Layer: Amazon Aurora PostgreSQL Serverless v2 with writer and reader instances, supporting automatic scaling and multi-AZ high availability. Caching Layer: Amazon ElastiCache Serverless (Redis 7.1) for session caching and frequently accessed data. Authentication: Amazon Cognito manages user registration, JWT-based authentication, and optional MFA, integrated with API Gateway. Storage \u0026amp; Content Delivery: Frontend hosted on Amazon S3 and distributed via CloudFront, protected by AWS WAF and ACM SSL/TLS certificates. Secrets \u0026amp; Monitoring: AWS Secrets Manager for storing credentials (DB, Redis, JWT keys) with 30-day rotation. Amazon CloudWatch for logging, metrics, and scaling alarms. 5. Timeline \u0026amp; Milestones Project Timeline\nPhase 1: Foundation \u0026amp; Design (Weeks 1-2) Week 1: Finalize MVP scope (P0 User Stories), define user flows, and approve the AWS architecture. Week 2: FE Lead finalizes UI/UX mockups. Backend provisions core AWS (VPC, S3, ECR, Aurora). Phase 2: Core MVP Development (Weeks 3-8) Weeks 3-4: Backend builds User Auth (Cognito) \u0026amp; core APIs (API Gateway, ECS). Weeks 5-6: All teams (FE/BE/Mobile) build core screens (Login, Search, Details) and the Booking Engine APIs. Weeks 7-8: Integration of KYC flow (Lambda, Textract, Rekognition) and Payment Gateway integration. Phase 3: Testing \u0026amp; UAT (Weeks 9-10) Week 9: Full End-to-End (E2E) testing. QA is performed by the 5-person dev team, as no dedicated QA is allocated. Week 10: Stakeholder User Acceptance Testing (UAT) and final critical bug fixing. Phase 4: Launch (Week 11) Week 11: Production deployment, Go-live, and intensive Hypercare monitoring via CloudWatch. 6. Budget Estimation This budget estimate is based on the provided AWS architecture diagram and the \u0026ldquo;cheapest possible\u0026rdquo; MVP launch strategy, maximizing Free Tier usage.\nInfrastructure Costs AWS Services (Monthly Estimate): Amazon Route 53: $0.50/month (1 hosted zone). AWS S3 Standard: $0.00/month (Stays within 5GB Always Free tier). Amazon CloudFront: $0.00/month (Stays within 1TB/10M request Always Free tier). AWS Cognito: $0.00/month (Stays within 10,000 MAU free tier). Amazon API Gateway: $0.00/month (Stays within 1M request 12-month free tier). AWS Lambda: $0.00/month (Stays within 1M request Always Free tier). Amazon Textract/Rekognition: $0.00/month (Stays within 12-month free tier for KYC). Application Load Balancer: $17.52/month (1 ALB, minimal processing). VPC Endpoint (PrivateLink): $7.30/month (1 Endpoint, 1 AZ, 1GB data). Amazon ECS on Fargate: ~$20.00/month (Assumes 2 minimal 24/7 containers, e.g., 0.25 vCPU/0.5GB RAM). Amazon RDS: ~$25.00/month (Minimal ACUs, configured to scale to near-zero). Amazon ElastiCache Serverless: ~$10.00/month (Minimal usage). Amazon CloudWatch: $0.00/month (Stays within 5GB log Always Free tier). Amazon ECR: ~$0.10/month (Minimal storage over 500MB free tier). Total: ~$86.42/month, ~$1,037.04/12 months\n7. Risk Assessment Risk Matrix System Downtime: High impact, medium probability. Data Sync Errors (Between Stations \u0026amp; Server): Medium impact, high probability. OCR Verification Failure: Medium impact, medium probability. Vehicle Shortage or Low Battery at Stations: High impact, high probability. Operational Mistakes by Staff: Medium impact, medium probability. Cost Overruns: Medium impact, low probability. Mitigation Strategies System: Use load-balanced cloud servers with auto-scaling and failover backup. Data Sync: Implement offline caching and periodic background synchronization. OCR Verification: Combine AI-based ID recognition with manual approval option. Vehicle Management: Real-time tracking of battery and vehicle status; predictive restocking via analytics. Staff Operations: Provide training and digital checklists to reduce human error. Cost: Set up cloud cost monitoring and optimization alerts. Contingency Plans Enable offline mode for station staff when Internet is unavailable. Activate backup servers in case of major downtime. Provide manual check-in/out workflow for rentals during system outages. Deploy mobile maintenance team to handle vehicle or battery issues at stations. Suspend or limit reservations dynamically if vehicle supply falls below safe threshold. 8. Expected Outcomes Technical Improvements: Real-time monitoring of all EV stations and rental status. Automated verification and e-contract signing replace manual paperwork. Centralized dashboard for admins to manage fleet, customers, and staff. System scalable to 20+ rental stations in the next deployment phase. Long-term Value Establishes a reliable EV mobility infrastructure for urban areas. Builds data foundation for future AI-powered demand forecasting. Enables integration with smart city and green transportation networks. Serves as a reusable platform for expanding to nationwide EV-sharing projects. Short to Medium-term Benefits Faster customer onboarding (from 15 mins → \u0026lt;5 mins). Increased fleet utilization rate by 30% through data-driven scheduling. Improved accuracy of rental records and payment reconciliation. Enhanced user satisfaction via seamless booking and transparent billing. "},{"uri":"https://khaicoderlor.github.io/intership-report-aws/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"Summary Report: AWS Cloud Mastery Series #2 – DevOps, IaC \u0026amp; Container Observability Event Objectives Build DevOps Mindset: Reshape understanding of the Value Cycle, affirming DevOps role in ensuring fast and reliable software delivery processes Automate Infrastructure (IaC): Eliminate manual management (ClickOps), transition to code-based infrastructure management through CloudFormation, Terraform, and CDK Optimize Container Strategy: Master architecture and selection criteria for appropriate orchestration platforms: App Runner for simplicity, ECS for deep integration, and EKS for open systems Proactive Monitoring (Observability): Establish comprehensive monitoring systems, focusing on early incident detection and performance optimization with CloudWatch and X-Ray Speakers AWS \u0026amp; Cloud Engineers Expert Team: Sharing practical knowledge about system architecture, Platform Engineering mindset, and technical demonstrations Key Highlights DevOps Mindset \u0026amp; CI/CD Pipeline Value Cycle thinking: 5-step closed-loop process from Insights → Backlog → CI → Testing → Delivery. Goal is to balance Release Speed and System Stability Standardizing CI/CD concepts: CI (Continuous Integration): Daily code integration, Fail fast with automated Unit Tests CD (Continuous Delivery): Auto-deploy to Staging, requires Manual Approval for Production Continuous Deployment: 100% fully automated Zero-touch process Pipeline Strategy: Artifact Management: Follow \u0026ldquo;Build Once, Deploy Anywhere\u0026rdquo; principle. Build source code once into Binary package and reuse for all environments to ensure consistency Stop the Line: Pipeline must halt immediately upon detecting security issues, Code Style violations, or test failures Metrics: Monitor system health through Deployment Frequency, Change Failure Rate, and MTTR Infrastructure as Code (IaC) - End of ClickOps Era ClickOps Limitations: Manual Console operations lead to errors, difficult scaling, and configuration drift IaC Power: Automation, accurate environment recreation, and infrastructure version control Toolset Analysis: AWS CloudFormation: Native tool using YAML/JSON. Manages resources by Stack units Terraform: Open-source solution for Multi-cloud. Write → Plan → Apply workflow enables safe change control before execution. State management via State File AWS CDK: Write infrastructure in programming languages (Python, TS\u0026hellip;). Use Constructs (L2/L3) to build complex architecture with few lines following Best Practices Drift Detection: Feature to detect configuration discrepancies between Code and Reality, ensuring operational discipline Containerization - Application Operation Strategy Orchestrator Selection: Kubernetes (K8s) / Amazon EKS: Powerful, open ecosystem, suitable for Enterprise or complex systems requiring high customization Amazon ECS: Simplified operations, deep integration with other AWS services (ALB, IAM) Compute Models: EC2 Launch Type: Maximum infrastructure control but requires management overhead (Patching, Scaling) AWS Fargate (Serverless): Eliminates server management burden, focus only on Task Resource definition AWS App Runner: \u0026ldquo;Zero-ops\u0026rdquo; solution optimal for Web App/API, automates from Source/Image to HTTPS URL Observability - Monitoring \u0026amp; Optimization Amazon CloudWatch: Monitoring hub with Metrics (Performance), Logs (Centralized logging), and Alarms (Auto alerts \u0026amp; Trigger actions) AWS X-Ray: Solves Distributed Tracing in Microservices architecture, helps identify Bottlenecks and root causes Best Practices: Clearly distinguish roles of Logs (individual events) and Traces (journey chains) to optimize debugging capability Key Takeaways Design mindset Platform Engineering: Shifting DevOps role from manual executor to builder of Self-service Platform with Governance control Operational Discipline: Iron discipline in operations - No manual changes on code-managed environments Technical architecture Consistency: Deep understanding of consistency importance across environments through proper Artifact management Smart Tooling: Skills to choose tools based on actual needs (CloudFormation for native stability, Terraform for multi-platform, CDK for Developer flexibility) Operational strategy Automation First: Prioritize automation of everything from infrastructure Provisioning to application Deployment Observability-Driven: Systems need not just to run but to be \u0026ldquo;visible\u0026rdquo; inside to proactively handle issues before affecting users Applying to Work Standardize processes: Review current Pipeline, apply Artifact Promotion mechanism (Build once, Deploy everywhere) Establish Centralized CI while empowering Dev team to avoid bottlenecks Modernize infrastructure: Start converting manually-created resources to Terraform/CDK Use Drift Detection periodically to check configuration compliance Optimize deployment: Re-evaluate current workload, consider moving simple Web Services to App Runner or ECS Fargate to reduce operational overhead Enhance monitoring: Integrate X-Ray into critical services to visualize request flow and optimize latency Event Experience Participating in this session was a major turning point helping me systematize all knowledge about modern Cloud operations.\nMindset shift about \u0026ldquo;Ops\u0026rdquo; I realized modern DevOps is not \u0026ldquo;server babysitters\u0026rdquo;. DevOps are architects building \u0026ldquo;highways\u0026rdquo; helping Developers deliver products to customers fastest and safest The concept of Self-service combined with Governance truly resolves the eternal contradiction between development speed and system stability Lesson about discipline Sharing about Artifact Management and Configuration Drift helped me understand why large systems need strictness in processes. Adhering to \u0026ldquo;Build Once\u0026rdquo; and \u0026ldquo;Infrastructure as Code\u0026rdquo; is the key to eliminating human errors Flexibility in technology Clear analysis of CloudFormation, Terraform, and CDK pros/cons gave me a more multi-dimensional view. There\u0026rsquo;s no best tool, only the most suitable tool for project context and team skills I was particularly impressed with the simplification of App Runner and Fargate, helping teams focus on business logic instead of getting lost in server management Conclusion The session provided a clear roadmap: Automation mindset → Infrastructure as Code (IaC) → Flexible operations (Container) → Deep monitoring (Observability). This is the success formula for any modern system on AWS Overall, the event not only provided technical knowledge but also helped me change my thinking about system operations, automation, and building modern DevOps platforms.\n"},{"uri":"https://khaicoderlor.github.io/intership-report-aws/5-workshop/5.3-setup-bedrock/","title":"Setup AWS Bedrock","tags":[],"description":"","content":"Setting up AWS Bedrock \u0026amp; Knowledge Base In this section, you will configure AWS Bedrock to use Claude 3.5 Sonnet and create a Knowledge Base for document retrieval.\nStep 1: Enable Model Access IMPORTANT: You must enable model access before using Bedrock, otherwise you\u0026rsquo;ll get ValidationException errors.\nGo to AWS Console → Services → Bedrock In the left sidebar, click Model access (under Foundation models) Click Manage model access button (orange) Find and select these models: ✅ Anthropic - Claude 3.5 Sonnet v2 (anthropic.claude-3-5-sonnet-20241022-v2:0) ✅ Amazon - Titan Embeddings G1 - Text (for Knowledge Base) Click Request model access (bottom right) Wait for approval: Instant access models: Available immediately (green ✅) Other models: Wait 5-30 minutes (status changes from \u0026ldquo;In progress\u0026rdquo; → \u0026ldquo;Access granted\u0026rdquo;) Verify models are enabled:\n# Using AWS CLI aws bedrock list-foundation-models --region us-west-2 # Or check in Console: # Bedrock → Model access → Status must be \u0026#34;Access granted\u0026#34; Step 2: Create S3 Bucket for Knowledge Base Knowledge Base requires an S3 bucket to store documents.\nGo to S3 → Create bucket Bucket name: ev-rental-knowledge-docs (must be globally unique) Region: Same as your Bedrock region (e.g., us-west-2) Block all public access: ✅ Enabled (recommended) Click Create bucket Step 3: Upload Documents to S3 Upload your rental policy documents (PDF, TXT, DOCX):\nSample documents to upload:\nrental-policy.pdf - Rental policies and terms pricing.pdf - Vehicle pricing information faq.txt - Frequently asked questions booking-process.pdf - How to book a vehicle Upload via Console:\nGo to your S3 bucket: ev-rental-knowledge-docs Click Upload → Add files Select your documents Click Upload Upload via AWS CLI:\naws s3 cp rental-policy.pdf s3://ev-rental-knowledge-docs/ aws s3 cp pricing.pdf s3://ev-rental-knowledge-docs/ aws s3 cp faq.txt s3://ev-rental-knowledge-docs/ aws s3 cp booking-process.pdf s3://ev-rental-knowledge-docs/ Step 4: Create Knowledge Base Go to Bedrock → Knowledge Bases → Create Knowledge base name: ev-rental-knowledge-base Description: \u0026ldquo;VinFast EV rental policies and FAQs\u0026rdquo; Click Next Data source configuration:\nData source name: rental-docs S3 URI: s3://ev-rental-knowledge-docs/ Click Next Embeddings model:\nSelect: Titan Embeddings G1 - Text (amazon.titan-embed-text-v1) Vector database: Choose Bedrock managed (OpenSearch Serverless) (easiest option) Click Next Review and create:\nReview all settings Click Create knowledge base Wait for creation to complete (2-3 minutes) Step 5: Sync Data Source After Knowledge Base is created, you need to sync the data:\nIn your Knowledge Base, go to Data sources tab Select your data source: rental-docs Click Sync button Wait for sync to complete (check status: \u0026ldquo;Syncing\u0026rdquo; → \u0026ldquo;Ready\u0026rdquo;) This process indexes all documents and creates vector embeddings Sync status:\n🔄 Syncing: In progress ✅ Ready: Completed successfully ❌ Failed: Check S3 permissions or document formats Step 6: Get Knowledge Base ID You\u0026rsquo;ll need this ID for your backend application:\nIn your Knowledge Base page Copy the Knowledge Base ID (format: 89CI1JSSE4 or similar) Save it in your notes - you\u0026rsquo;ll use it in the next step Example Knowledge Base ID:\nKnowledge Base ID: 89CI1JSSE4 Knowledge Base ARN: arn:aws:bedrock:us-west-2:123456789:knowledge-base/89CI1JSSE4 Step 7: Test Knowledge Base (Optional) Test your Knowledge Base directly in the console:\nGo to your Knowledge Base Click Test tab Enter a question: \u0026ldquo;What is the rental policy?\u0026rdquo; Click Run Verify it returns relevant information from your documents Verification Checklist Before moving to the next step, ensure:\n✅ Claude 3.5 Sonnet v2 model access is granted ✅ Titan Embeddings model access is granted ✅ S3 bucket created with documents uploaded ✅ Knowledge Base created and data synced successfully ✅ Knowledge Base ID saved ✅ Test query returns relevant results Troubleshooting Issue: \u0026ldquo;ValidationException: Model not enabled\u0026rdquo;\nSolution: Go to Bedrock → Model access and enable the model Issue: \u0026ldquo;Sync failed\u0026rdquo;\nCheck S3 bucket permissions Verify document formats (PDF, TXT, DOCX supported) Check CloudWatch Logs for detailed errors Issue: \u0026ldquo;No results from Knowledge Base\u0026rdquo;\nEnsure documents are uploaded to S3 Run sync again Wait a few minutes after sync completes Try different query phrasing Next: Proceed to Deploy Backend API to build the FastAPI server.\n"},{"uri":"https://khaicoderlor.github.io/intership-report-aws/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nThis section will list and introduce the blogs you have translated. For example:\nBlog 1 - Getting started with healthcare data lakes: Using microservices This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 2 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 3 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 4 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 5 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 6 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"},{"uri":"https://khaicoderlor.github.io/intership-report-aws/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Master IAM advanced features for secure access control Learn RDS database services and configurations Understand Lambda serverless computing Introduction to Infrastructure as Code with CloudFormation Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Mon IAM Advanced - Roles \u0026amp; Policies: - IAM roles for EC2, Lambda, cross-account access - Custom policies vs AWS managed policies - Service Control Policies (SCPs) Practice: Create custom IAM policies, implement role assumption 22/09/2025 22/09/2025 https://cloudjourney.awsstudygroup.com/ Tue RDS Fundamentals: - RDS engines: MySQL, PostgreSQL, MariaDB, Oracle, SQL Server - Multi-AZ deployments for HA - Read replicas for scaling Practice: Launch RDS instance, configure backups 23/09/2025 23/09/2025 https://cloudjourney.awsstudygroup.com/ Wed RDS Advanced \u0026amp; Aurora: - Aurora architecture and benefits - Parameter groups and option groups - Encryption and security Practice: Deploy Aurora cluster, test failover 24/09/2025 24/09/2025 https://cloudjourney.awsstudygroup.com/ Thu Lambda Serverless: - Lambda functions and runtimes - Event sources and triggers - Lambda layers and environment variables Practice: Create Lambda function, integrate with S3 events 25/09/2025 25/09/2025 https://cloudjourney.awsstudygroup.com/ Fri CloudFormation IaC: - CloudFormation templates (YAML/JSON) - Stacks, resources, parameters, outputs - Change sets and drift detection Practice: Write template to deploy VPC and EC2 26/09/2025 26/09/2025 https://cloudjourney.awsstudygroup.com/ Week 3 Achievements: IAM Expertise:\nCreated complex custom IAM policies with conditions Implemented cross-account role assumption Understood policy evaluation logic and precedence Configured service-linked roles Database Skills:\nDeployed RDS instance with Multi-AZ configuration Configured automated backups and snapshots Created read replicas for read-heavy workloads Understood RDS parameter groups for tuning Aurora Knowledge:\nDeployed Aurora cluster with high availability Tested automatic failover mechanisms Understood Aurora Serverless for variable workloads Compared Aurora vs traditional RDS performance Serverless Foundation:\nCreated first Lambda function in Python/Node.js Configured S3 event triggers for Lambda Understood Lambda pricing and cold starts Implemented error handling and logging Infrastructure as Code:\nWrote CloudFormation templates for VPC Deployed complete infrastructure stack Understood stack updates and rollbacks Practiced template parameterization **S3 Mastery:\nUnderstood all storage classes and their cost implications Successfully configured lifecycle policies to automatically transition objects Deployed a static website on S3 with custom domain Implemented versioning and object recovery Security Enhancement:\nMastered IAM policy syntax and evaluation logic Configured cross-account access using STS Implemented least privilege principle in production Understood the difference between user-based and resource-based policies Authentication:\nSuccessfully set up Cognito user pool Integrated basic authentication flows Understood JWT token structure and validation Configured identity pool for AWS credential provisioning Database Foundation:\nCreated first RDS instance with proper security groups Connected application to RDS from EC2 Understood Multi-AZ for high availability Learned about automated backups and snapshots Best Practices:\nAlways enable encryption at rest for S3 and RDS Use IAM roles instead of hardcoded credentials Implement proper logging with CloudTrail Tag all resources for cost allocation "},{"uri":"https://khaicoderlor.github.io/intership-report-aws/5-workshop/5.4-deploy-backend/","title":"Deploy Backend API","tags":[],"description":"","content":"Deploy Backend API with FastAPI In this section, you will set up the FastAPI backend server that orchestrates the AI agent using Strands SDK.\nStep 1: Clone or Create Project Structure Create a new directory for the backend:\nmkdir ev-rental-backend cd ev-rental-backend Project structure:\nev-rental-backend/ ├── app/ │ ├── __init__.py │ ├── main.py # FastAPI app │ ├── agent.py # Strands Agent setup │ ├── tools.py # Agent tools (search vehicles, stations) │ └── database.py # PostgreSQL connection ├── requirements.txt # Python dependencies ├── .env # Environment variables └── README.md Step 2: Install Dependencies Create requirements.txt:\nfastapi==0.104.1 uvicorn[standard]==0.24.0 strands-agent-sdk==0.1.5 boto3==1.34.10 psycopg2-binary==2.9.9 sqlalchemy==2.0.23 pydantic==2.5.2 python-dotenv==1.0.0 httpx==0.25.2 Install dependencies:\n# Create virtual environment python -m venv venv # Activate virtual environment # Windows: venv\\Scripts\\activate # macOS/Linux: source venv/bin/activate # Install packages pip install -r requirements.txt Step 3: Configure Environment Variables Create .env file with your AWS credentials and Knowledge Base ID:\n# AWS Credentials AWS_ACCESS_KEY_ID=YOUR_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY=YOUR_SECRET_ACCESS_KEY AWS_REGION=us-west-2 # Bedrock Configuration BEDROCK_MODEL_ID=anthropic.claude-3-5-sonnet-20241022-v2:0 KNOWLEDGE_BASE_ID=89CI1JSSE4 # Database Configuration DATABASE_URL=postgresql://postgres:password@localhost:5432/ev_rental_db # API Configuration BACKEND_API_URL=http://localhost:8080 ⚠️ Security Note:\nNever commit .env to Git Add .env to .gitignore Step 4: Create Database Models Create app/database.py:\nfrom sqlalchemy import create_engine, Column, Integer, String, Text, DateTime from sqlalchemy.ext.declarative import declarative_base from sqlalchemy.orm import sessionmaker from datetime import datetime import os DATABASE_URL = os.getenv(\u0026#34;DATABASE_URL\u0026#34;) engine = create_engine(DATABASE_URL) SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine) Base = declarative_base() class ChatHistory(Base): __tablename__ = \u0026#34;chat_history\u0026#34; id = Column(Integer, primary_key=True, index=True) session_id = Column(String, index=True) user_message = Column(Text) agent_response = Column(Text) timestamp = Column(DateTime, default=datetime.utcnow) # Create tables Base.metadata.create_all(bind=engine) Step 5: Create Agent Tools Create app/tools.py:\nimport httpx import os from typing import List, Dict BACKEND_API_URL = os.getenv(\u0026#34;BACKEND_API_URL\u0026#34;, \u0026#34;http://localhost:8080\u0026#34;) async def search_vehicles(location: str = None, model: str = None) -\u0026gt; List[Dict]: \u0026#34;\u0026#34;\u0026#34;Search for available vehicles\u0026#34;\u0026#34;\u0026#34; async with httpx.AsyncClient() as client: params = {} if location: params[\u0026#34;location\u0026#34;] = location if model: params[\u0026#34;model\u0026#34;] = model response = await client.get(f\u0026#34;{BACKEND_API_URL}/api/vehicles\u0026#34;, params=params) return response.json() async def search_stations(city: str = None) -\u0026gt; List[Dict]: \u0026#34;\u0026#34;\u0026#34;Search for charging stations\u0026#34;\u0026#34;\u0026#34; async with httpx.AsyncClient() as client: params = {} if city: params[\u0026#34;city\u0026#34;] = city response = await client.get(f\u0026#34;{BACKEND_API_URL}/api/stations\u0026#34;, params=params) return response.json() Step 6: Setup Strands Agent Create app/agent.py:\nimport boto3 import os from strands_agent import Agent, Tool # Initialize Bedrock client bedrock_client = boto3.client( \u0026#39;bedrock-runtime\u0026#39;, region_name=os.getenv(\u0026#39;AWS_REGION\u0026#39;), aws_access_key_id=os.getenv(\u0026#39;AWS_ACCESS_KEY_ID\u0026#39;), aws_secret_access_key=os.getenv(\u0026#39;AWS_SECRET_ACCESS_KEY\u0026#39;) ) # Initialize Knowledge Base client bedrock_agent_client = boto3.client( \u0026#39;bedrock-agent-runtime\u0026#39;, region_name=os.getenv(\u0026#39;AWS_REGION\u0026#39;), aws_access_key_id=os.getenv(\u0026#39;AWS_ACCESS_KEY_ID\u0026#39;), aws_secret_access_key=os.getenv(\u0026#39;AWS_SECRET_ACCESS_KEY\u0026#39;) ) # Create Agent agent = Agent( model_id=os.getenv(\u0026#39;BEDROCK_MODEL_ID\u0026#39;), client=bedrock_client, knowledge_base_id=os.getenv(\u0026#39;KNOWLEDGE_BASE_ID\u0026#39;), tools=[ Tool( name=\u0026#34;search_vehicles\u0026#34;, description=\u0026#34;Search for available electric vehicles for rent\u0026#34;, function=search_vehicles ), Tool( name=\u0026#34;search_stations\u0026#34;, description=\u0026#34;Find nearby charging stations\u0026#34;, function=search_stations ) ] ) Step 7: Create FastAPI Application Create app/main.py:\nfrom fastapi import FastAPI, HTTPException from fastapi.middleware.cors import CORSMiddleware from pydantic import BaseModel from app.agent import agent from app.database import SessionLocal, ChatHistory import uuid app = FastAPI(title=\u0026#34;EV Rental AI Agent API\u0026#34;) # Enable CORS app.add_middleware( CORSMiddleware, allow_origins=[\u0026#34;*\u0026#34;], allow_credentials=True, allow_methods=[\u0026#34;*\u0026#34;], allow_headers=[\u0026#34;*\u0026#34;], ) class ChatRequest(BaseModel): message: str session_id: str = None class ChatResponse(BaseModel): response: str session_id: str data: dict = None @app.post(\u0026#34;/chat\u0026#34;, response_model=ChatResponse) async def chat(request: ChatRequest): try: # Generate session ID if not provided session_id = request.session_id or str(uuid.uuid4()) # Get agent response agent_response = await agent.run(request.message) # Save to database db = SessionLocal() chat_record = ChatHistory( session_id=session_id, user_message=request.message, agent_response=agent_response[\u0026#34;response\u0026#34;] ) db.add(chat_record) db.commit() db.close() return ChatResponse( response=agent_response[\u0026#34;response\u0026#34;], session_id=session_id, data=agent_response.get(\u0026#34;data\u0026#34;) ) except Exception as e: raise HTTPException(status_code=500, detail=str(e)) @app.get(\u0026#34;/health\u0026#34;) async def health_check(): return {\u0026#34;status\u0026#34;: \u0026#34;healthy\u0026#34;} Step 8: Run the Backend Server Start the FastAPI server:\n# Make sure virtual environment is activated uvicorn app.main:app --reload --port 8000 # You should see: # INFO: Uvicorn running on http://127.0.0.1:8000 # INFO: Application startup complete. Step 9: Test the API Test health endpoint:\ncurl http://localhost:8000/health # Response: {\u0026#34;status\u0026#34;:\u0026#34;healthy\u0026#34;} Test chat endpoint:\ncurl -X POST http://localhost:8000/chat \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;message\u0026#34;: \u0026#34;Chính sách thuê xe của bạn là gì?\u0026#34;}\u0026#39; Expected response:\n{ \u0026#34;response\u0026#34;: \u0026#34;## 📋 Chính sách thuê xe VinFast\\n\\n### 📄 Giấy tờ cần thiết:\\n- CMND/CCCD...\u0026#34;, \u0026#34;session_id\u0026#34;: \u0026#34;abc123-...\u0026#34;, \u0026#34;data\u0026#34;: null } Verification Checklist Before proceeding, ensure:\n✅ Virtual environment created and activated ✅ All dependencies installed ✅ .env file configured with AWS credentials ✅ PostgreSQL database running and connected ✅ FastAPI server running on port 8000 ✅ Health check endpoint returns {\u0026quot;status\u0026quot;:\u0026quot;healthy\u0026quot;} ✅ Chat endpoint returns proper responses Troubleshooting Issue: \u0026ldquo;ModuleNotFoundError\u0026rdquo;\nSolution: Ensure virtual environment is activated and dependencies installed Issue: \u0026ldquo;Database connection failed\u0026rdquo;\nCheck PostgreSQL is running Verify DATABASE_URL in .env Test connection: psql -h localhost -U postgres -d ev_rental_db Issue: \u0026ldquo;Bedrock ValidationException\u0026rdquo;\nVerify AWS credentials in .env Ensure model access is granted in Bedrock console Check KNOWLEDGE_BASE_ID is correct Next: Proceed to Deploy Frontend to create the React chat interface.\n"},{"uri":"https://khaicoderlor.github.io/intership-report-aws/4-eventparticipated/4.4-event4/","title":"Event 4","tags":[],"description":"","content":"Summary Report: AWS Cloud Mastery Series #3 – Cloud Security \u0026amp; Operations Mastery Event Series Objectives Transform governance mindset: Shift from traditional perimeter security model to Cloud-Native Security thinking with Systems Thinking approach Build defense systems: Establish Defense in Depth strategy, ensuring security from Identity, Network to Data layers Automate response: Minimize human risk by establishing Automated Response processes for immediate incident handling Standardize large-scale governance: Establish strong Governance foundation to manage hundreds of AWS accounts while ensuring Compliance Speakers AWS Cloud Clubs Representatives: Le Vu Xuan An, Tran Duc Anh, Tran Doan Cong Ly, Danh Hoang Hieu Nghi (Captains from universities) Identity \u0026amp; Governance Track: Huynh Hoang Long, Dinh Le Hoang Anh (AWS Community Builders) Detection \u0026amp; Monitoring Track: Tran Duc Anh, Nguyen Tuan Thinh, Nguyen Do Thanh Dat Network Security Track: Kha Van (Cloud Security Engineer) Data Protection Track: Thinh Lam, Viet Nguyen Incident Response Track: Mendel Grabski (Security Expert), Tinh Truong (Platform Engineer) Detailed Content Launch with AWS Cloud Clubs Vision: Nurturing future Cloud talent, focusing on leadership skills development and global community connection Core values: Hands-on opportunities through projects (Build Skills), network expansion (Build Community), and access to exclusive career resources (Build Opportunities) The Badging Journey: Gamified development path from Bronze to Diamond levels with attractive benefits like AWS Credits and certification vouchers Identity \u0026amp; Governance Foundation Identity as the new Firewall: In Cloud environments, identity is the most critical protection layer Credential Spectrum: Thoroughly apply Short-term Credentials (auto-expiring STS tokens) instead of permanent Access Keys with inherent risks Governance principles: Least Privilege: Grant minimum necessary permissions, limit wildcard (*) usage AWS Organizations \u0026amp; SCPs: Use Service Control Policies (SCPs) as a \u0026ldquo;Digital Constitution\u0026rdquo;, establishing Guardrails to prevent dangerous actions organization-wide (e.g., prohibit disabling logging, restrict access outside allowed geographic regions) Visibility \u0026amp; Detection Capabilities Amazon GuardDuty: Intelligent reconnaissance system using ML to analyze anomalies from CloudTrail, VPC Flow Logs, and DNS Logs. Runtime Monitoring feature enables deep detection of suspicious processes within operating systems AWS Security Hub: Command center helping standardize all alerts to ASFF format, giving administrators comprehensive view of security posture (CSPM) and international standards compliance levels (PCI-DSS, CIS) Network Security Micro-segmentation strategy: Use Security Groups with Referencing mechanism instead of static IP management, increasing application flexibility and security Advanced defense: DNS Firewall: Block connections to hacker C2 (Command \u0026amp; Control) servers at domain resolution stage AWS Network Firewall: Next-generation firewall with Deep Packet Inspection capability, integrated IPS (Suricata) to filter malicious traffic AWS Transit Gateway: Simplify network architecture, combined with automatic IP Blacklist synchronization from GuardDuty to block real-time threats Data Protection Envelope Encryption: Optimize performance and security through AWS KMS\u0026rsquo;s layered key mechanism (Master Key protects Data Key) Secrets Management: Completely eliminate hardcoded passwords using AWS Secrets Manager, integrated with Automatic Rotation capability to minimize exposure risks Hardware encryption: Leverage AWS Nitro System to perform encryption with Zero Performance Impact on server performance Incident Response Prevention strategy: Apply Infrastructure as Code (IaC) discipline to eliminate configuration errors from manual operations (ClickOps). Remove common weaknesses like open SSH ports or public S3 buckets Standard process: Follow 5-step response lifecycle: Prepare → Detect → Contain → Eradicate/Recover → Post-incident Automation is King: Use EventBridge and Lambda to automate response tasks like isolating malware-infected servers in seconds, instead of waiting for human intervention Key Takeaways Design mindset Governance First: All secure systems start with strict identity and policy management Defense in Depth: Don\u0026rsquo;t rely on single protection layer; combine network security, behavior monitoring, and data encryption Technical architecture Identity-First Security: Identity is the new security boundary in Cloud era, replacing traditional firewalls Automation \u0026amp; Detection: Automate incident detection and response to minimize Mean Time To Resolution (MTTR) Operational strategy Resilience: Be prepared for worst-case scenarios with highly automated incident response processes Compliance as Code: Use SCPs and Security Hub to ensure continuous compliance at scale Applying to Work Strengthen Identity foundation: Implement strict Least Privilege policy, eliminate long-lived Access Keys Establish SCPs to create organization-wide guardrails Enhance detection capabilities: Activate GuardDuty and Security Hub for comprehensive security posture view Set up automated alerts for critical events Optimize network security: Apply micro-segmentation with Security Groups following best practices Deploy DNS Firewall and Network Firewall for production environments Protect sensitive data: Transition to AWS Secrets Manager instead of hardcoded credentials Enable encryption for all data at-rest and in-transit Prepare incident response: Build automated playbooks with EventBridge and Lambda Conduct regular Incident Response Drills Event Experience Participating in the \u0026ldquo;Cloud Security \u0026amp; Operations Mastery\u0026rdquo; series was a truly transformative experience in modern security thinking.\nNew perspective on Cloud security Cloud security isn\u0026rsquo;t about \u0026ldquo;locking doors tight\u0026rdquo;, but building a multi-layered defense ecosystem with self-adjustment capabilities The concept of \u0026ldquo;Identity as the new Firewall\u0026rdquo; truly changed how I design security architecture, shifting focus from network perimeter to identity management Power of automation Demos on automated incident response using EventBridge and Lambda showed the ability to reduce MTTR from hours to seconds Integration of GuardDuty with Transit Gateway to automatically block malicious IPs is a very practical and effective solution Importance of Governance SCPs are not just permission management tools but \u0026ldquo;safety guardrails\u0026rdquo; preventing dangerous actions at organizational level Security Hub helps standardize security posture assessment and measurement, especially critical when managing multiple AWS accounts Experience with AWS Cloud Clubs Impressed by AWS\u0026rsquo;s investment in student community through Cloud Clubs program The Badging Journey is an interesting approach to encourage continuous learning and skill development Conclusion The event provided a complete Cloud security framework: from Identity \u0026amp; Governance, Detection \u0026amp; Monitoring, Network Security, Data Protection to Incident Response Realized that security is not a one-time task but a continuous journey of improvement and adaptation to new threats Automation and defense in depth are two indispensable pillars in modern security strategy Overall, the event not only provided technical knowledge but also helped me build systems thinking about Cloud security, enabling me to design and operate secure, flexible, and highly resilient systems.\n"},{"uri":"https://khaicoderlor.github.io/intership-report-aws/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim for your report, including this warning.\nIn this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3…, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event’s content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: GenAI-powered App-DB Modernization workshop\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: GenAI-powered App-DB Modernization workshop\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://khaicoderlor.github.io/intership-report-aws/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Master ElastiCache for in-memory caching strategies Understand AWS Load Balancers (ALB, NLB, CLB, GWLB) Introduction to Docker containerization Learn ECS, Fargate, and ECR for container management Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Mon ElastiCache Redis: - Redis vs Memcached comparison - Caching patterns: cache-aside, read-through, write-through - ElastiCache cluster setup and configuration Practice: Deploy Redis cluster, implement cache-aside pattern 29/09/2025 29/09/2025 https://cloudjourney.awsstudygroup.com/ Tue Elastic Load Balancing (ELB): - ALB vs NLB vs CLB vs GWLB comparison - Target groups, health checks, listeners - Path-based and host-based routing Practice: Deploy ALB with multiple target groups 30/09/2025 30/09/2025 https://cloudjourney.awsstudygroup.com/ Wed Docker Basics: - Docker images, containers, Dockerfile - Docker networking and volumes - Multi-stage builds and best practices Practice: Build custom Docker image, run containers 01/10/2025 01/10/2025 https://cloudjourney.awsstudygroup.com/ Thu Amazon ECS \u0026amp; Fargate: - ECS clusters, task definitions, services - EC2 launch type vs Fargate serverless - Service discovery and load balancer integration Practice: Deploy containerized app on Fargate 02/10/2025 02/10/2025 https://cloudjourney.awsstudygroup.com/ Fri Amazon ECR: - ECR repositories and image management - Image scanning and vulnerability detection - Lifecycle policies for image retention Practice: Push Docker images to ECR, scan for vulnerabilities 03/10/2025 03/10/2025 https://cloudjourney.awsstudygroup.com/ Week 4 Achievements: Caching Expertise:\nDeployed ElastiCache Redis cluster with replication Implemented cache-aside pattern reducing DB load by 70% Configured eviction policies (LRU, LFU) and TTL management Understood Redis persistence modes (RDB, AOF) Load Balancing Mastery:\nDeployed ALB with path-based routing for microservices Configured target groups with health checks Understood NLB for TCP/UDP traffic and static IPs Compared GWLB for third-party appliances Docker Foundation:\nBuilt multi-stage Dockerfiles for optimized images Understood container networking modes (bridge, host, overlay) Implemented Docker volumes for persistent data Practiced image layering and caching strategies Container Orchestration:\nDeployed first containerized application on ECS Fargate Configured task definitions with CPU/memory allocation Integrated ECS services with ALB for traffic distribution Understood ECS service auto-scaling Container Registry:\nPushed Docker images to private ECR repositories Enabled image scanning for CVE vulnerabilities Configured lifecycle policies for automatic cleanup Practiced cross-region ECR replication "},{"uri":"https://khaicoderlor.github.io/intership-report-aws/5-workshop/5.5-deploy-frontend/","title":"Deploy Frontend","tags":[],"description":"","content":"Deploying the React Frontend In this section, you will set up and run the React chat interface that connects to your FastAPI backend.\nStep 1: Clone or Create React Project Create a new React application:\n# Using Create React App npx create-react-app ev-rental-frontend cd ev-rental-frontend # Or clone existing repository git clone https://github.com/your-org/ev-rental-frontend.git cd ev-rental-frontend Step 2: Install Dependencies Install required npm packages:\n# Core dependencies npm install axios react-markdown npm install @chakra-ui/react @emotion/react @emotion/styled framer-motion npm install react-icons # Or use package.json npm install Sample package.json dependencies:\n{ \u0026#34;dependencies\u0026#34;: { \u0026#34;react\u0026#34;: \u0026#34;^18.2.0\u0026#34;, \u0026#34;react-dom\u0026#34;: \u0026#34;^18.2.0\u0026#34;, \u0026#34;axios\u0026#34;: \u0026#34;^1.6.2\u0026#34;, \u0026#34;@chakra-ui/react\u0026#34;: \u0026#34;^2.8.2\u0026#34;, \u0026#34;@emotion/react\u0026#34;: \u0026#34;^11.11.1\u0026#34;, \u0026#34;@emotion/styled\u0026#34;: \u0026#34;^11.11.0\u0026#34;, \u0026#34;framer-motion\u0026#34;: \u0026#34;^10.16.16\u0026#34;, \u0026#34;react-markdown\u0026#34;: \u0026#34;^9.0.1\u0026#34;, \u0026#34;react-icons\u0026#34;: \u0026#34;^4.12.0\u0026#34; } } Step 3: Project Structure Your frontend should have this structure:\nev-rental-frontend/ ├── public/ │ ├── index.html │ └── favicon.ico ├── src/ │ ├── App.js # Main app component │ ├── index.js # Entry point │ ├── components/ │ │ ├── ChatInterface.js # Chat UI component │ │ ├── MessageList.js # Message display │ │ └── InputBox.js # User input │ ├── services/ │ │ └── api.js # API calls to backend │ ├── utils/ │ │ └── constants.js # Configuration │ └── styles/ │ └── App.css ├── package.json └── .env Step 4: Configure Environment Variables Create .env file in the project root:\n# .env REACT_APP_API_URL=http://localhost:8000 REACT_APP_API_BASE_PATH=/api ⚠️ Important: In React, environment variables must start with REACT_APP_ prefix.\nStep 5: Create API Service Create src/services/api.js:\nimport axios from \u0026#39;axios\u0026#39;; const API_URL = process.env.REACT_APP_API_URL || \u0026#39;http://localhost:8000\u0026#39;; const api = axios.create({ baseURL: API_URL, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, }, }); export const sendMessage = async (sessionId, message) =\u0026gt; { try { const response = await api.post(\u0026#39;/api/chat\u0026#39;, { session_id: sessionId, message: message, }); return response.data; } catch (error) { console.error(\u0026#39;API Error:\u0026#39;, error); throw error; } }; export default api; Step 6: Create Chat Interface Component Create src/components/ChatInterface.js:\nimport React, { useState, useEffect, useRef } from \u0026#39;react\u0026#39;; import { Box, VStack, HStack, Input, Button, Text, Container, Heading, } from \u0026#39;@chakra-ui/react\u0026#39;; import ReactMarkdown from \u0026#39;react-markdown\u0026#39;; import { sendMessage } from \u0026#39;../services/api\u0026#39;; function ChatInterface() { const [messages, setMessages] = useState([]); const [input, setInput] = useState(\u0026#39;\u0026#39;); const [loading, setLoading] = useState(false); const [sessionId] = useState(() =\u0026gt; `session-${Date.now()}-${Math.random().toString(36).substr(2, 9)}` ); const messagesEndRef = useRef(null); const scrollToBottom = () =\u0026gt; { messagesEndRef.current?.scrollIntoView({ behavior: \u0026#39;smooth\u0026#39; }); }; useEffect(() =\u0026gt; { scrollToBottom(); }, [messages]); const handleSend = async () =\u0026gt; { if (!input.trim()) return; const userMessage = { role: \u0026#39;user\u0026#39;, content: input }; setMessages((prev) =\u0026gt; [...prev, userMessage]); setInput(\u0026#39;\u0026#39;); setLoading(true); try { const response = await sendMessage(sessionId, input); const assistantMessage = { role: \u0026#39;assistant\u0026#39;, content: response.response, data: response.data, }; setMessages((prev) =\u0026gt; [...prev, assistantMessage]); } catch (error) { const errorMessage = { role: \u0026#39;error\u0026#39;, content: \u0026#39;Failed to get response. Please try again.\u0026#39;, }; setMessages((prev) =\u0026gt; [...prev, errorMessage]); } finally { setLoading(false); } }; return ( \u0026lt;Container maxW=\u0026#34;container.md\u0026#34; py={8}\u0026gt; \u0026lt;VStack spacing={4} align=\u0026#34;stretch\u0026#34;\u0026gt; \u0026lt;Heading size=\u0026#34;lg\u0026#34;\u0026gt;🚗 EV Rental AI Agent\u0026lt;/Heading\u0026gt; \u0026lt;Box border=\u0026#34;1px\u0026#34; borderColor=\u0026#34;gray.200\u0026#34; borderRadius=\u0026#34;lg\u0026#34; p={4} h=\u0026#34;500px\u0026#34; overflowY=\u0026#34;auto\u0026#34; bg=\u0026#34;gray.50\u0026#34; \u0026gt; \u0026lt;VStack spacing={3} align=\u0026#34;stretch\u0026#34;\u0026gt; {messages.map((msg, idx) =\u0026gt; ( \u0026lt;Box key={idx} alignSelf={msg.role === \u0026#39;user\u0026#39; ? \u0026#39;flex-end\u0026#39; : \u0026#39;flex-start\u0026#39;} maxW=\u0026#34;80%\u0026#34; bg={msg.role === \u0026#39;user\u0026#39; ? \u0026#39;blue.500\u0026#39; : \u0026#39;white\u0026#39;} color={msg.role === \u0026#39;user\u0026#39; ? \u0026#39;white\u0026#39; : \u0026#39;black\u0026#39;} p={3} borderRadius=\u0026#34;lg\u0026#34; boxShadow=\u0026#34;sm\u0026#34; \u0026gt; {msg.role === \u0026#39;assistant\u0026#39; ? ( \u0026lt;ReactMarkdown\u0026gt;{msg.content}\u0026lt;/ReactMarkdown\u0026gt; ) : ( \u0026lt;Text\u0026gt;{msg.content}\u0026lt;/Text\u0026gt; )} \u0026lt;/Box\u0026gt; ))} {loading \u0026amp;\u0026amp; ( \u0026lt;Box alignSelf=\u0026#34;flex-start\u0026#34; maxW=\u0026#34;80%\u0026#34;\u0026gt; \u0026lt;Text color=\u0026#34;gray.500\u0026#34;\u0026gt;Typing...\u0026lt;/Text\u0026gt; \u0026lt;/Box\u0026gt; )} \u0026lt;div ref={messagesEndRef} /\u0026gt; \u0026lt;/VStack\u0026gt; \u0026lt;/Box\u0026gt; \u0026lt;HStack\u0026gt; \u0026lt;Input value={input} onChange={(e) =\u0026gt; setInput(e.target.value)} onKeyPress={(e) =\u0026gt; e.key === \u0026#39;Enter\u0026#39; \u0026amp;\u0026amp; handleSend()} placeholder=\u0026#34;Ask about vehicle rentals, policies, or charging stations...\u0026#34; disabled={loading} /\u0026gt; \u0026lt;Button onClick={handleSend} colorScheme=\u0026#34;blue\u0026#34; isLoading={loading} disabled={loading} \u0026gt; Send \u0026lt;/Button\u0026gt; \u0026lt;/HStack\u0026gt; \u0026lt;/VStack\u0026gt; \u0026lt;/Container\u0026gt; ); } export default ChatInterface; Step 7: Update App.js Update src/App.js:\nimport React from \u0026#39;react\u0026#39;; import { ChakraProvider } from \u0026#39;@chakra-ui/react\u0026#39;; import ChatInterface from \u0026#39;./components/ChatInterface\u0026#39;; function App() { return ( \u0026lt;ChakraProvider\u0026gt; \u0026lt;ChatInterface /\u0026gt; \u0026lt;/ChakraProvider\u0026gt; ); } export default App; Step 8: Run the Frontend Start the React development server:\nnpm start Expected output:\nCompiled successfully! You can now view ev-rental-frontend in the browser. Local: http://localhost:3000 On Your Network: http://192.168.1.10:3000 The application will automatically open in your browser at http://localhost:3000.\nStep 9: Test the Chat Interface Try these sample queries:\nKnowledge Base Query:\n\u0026ldquo;Chính sách thuê xe là gì?\u0026rdquo; \u0026ldquo;Tôi cần giấy tờ gì để thuê xe?\u0026rdquo; Vehicle Search:\n\u0026ldquo;Tìm xe VinFast VF8 ở Hà Nội từ ngày 20/12\u0026rdquo; \u0026ldquo;Có xe nào available?\u0026rdquo; Charging Station:\n\u0026ldquo;Trạm sạc gần Hoàn Kiếm\u0026rdquo; \u0026ldquo;Tìm trạm sạc ở Quận 1\u0026rdquo; Verification Checklist Before proceeding, ensure:\n✅ Node.js and npm installed ✅ All dependencies installed successfully ✅ .env file configured with backend URL ✅ Backend server running on port 8000 ✅ Frontend running on port 3000 ✅ Chat interface loads without errors ✅ Can send messages and receive responses ✅ Markdown formatting displays correctly Troubleshooting Issue: \u0026ldquo;Module not found\u0026rdquo;\nSolution: Delete node_modules and run npm install again Check package.json for correct versions Issue: \u0026ldquo;Network Error\u0026rdquo; when sending messages\nCheck backend is running: curl http://localhost:8000/health Verify REACT_APP_API_URL in .env Check browser console for CORS errors Issue: \u0026ldquo;CORS policy error\u0026rdquo;\nEnsure backend has CORS middleware configured Check allow_origins includes http://localhost:3000 Issue: Port 3000 already in use\nChange port: PORT=3001 npm start Or kill existing process Issue: Markdown not rendering\nVerify react-markdown is installed Check import statement in ChatInterface.js Next: Proceed to Testing to verify all features work correctly.\n"},{"uri":"https://khaicoderlor.github.io/intership-report-aws/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Master Auto Scaling Groups and scaling policies Understand advanced ELB features with SSL/TLS Learn Route 53 DNS fundamentals and routing policies Practice VPC Peering for multi-VPC architectures Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Mon Auto Scaling Groups (ASG): - ASG launch templates and configurations - Scaling policies: target tracking, step, simple - ASG integration with ALB for high availability Practice: Deploy ASG + ALB, test auto scaling behavior 06/10/2025 06/10/2025 https://cloudjourney.awsstudygroup.com/ Tue ELB SSL/TLS: - Configure SSL/TLS certificates with ACM - Server Name Indication (SNI) for multi-domain hosting - SSL policy selection and security Practice: Configure HTTPS listener on ALB, import certificates 07/10/2025 07/10/2025 https://cloudjourney.awsstudygroup.com/ Wed Route 53 Basics: - DNS fundamentals and record types (A, AAAA, CNAME, MX) - Hosted zones: public vs private - Alias vs CNAME records Practice: Register domain, configure DNS records 08/10/2025 08/10/2025 https://cloudjourney.awsstudygroup.com/ Thu Route 53 Routing Policies: - Simple, Weighted, Latency-based routing - Failover and Geolocation routing - Multi-value answer routing Practice: Configure weighted routing for A/B testing 09/10/2025 09/10/2025 https://cloudjourney.awsstudygroup.com/ Fri VPC Peering: - VPC peering connections and route table updates - Inter-region VPC peering - Security group referencing across peered VPCs Practice: Create VPC peering, test cross-VPC connectivity 10/10/2025 10/10/2025 https://cloudjourney.awsstudygroup.com/ Week 5 Achievements: Auto Scaling Mastery:\nDeployed ASG with target tracking policy for CPU utilization Configured dynamic scaling based on CloudWatch metrics Integrated ASG with ALB for automatic instance registration Tested scale-out and scale-in behavior under load SSL/TLS Configuration:\nConfigured ACM certificates for domain validation Set up HTTPS listener on ALB with SSL termination Implemented SNI to host multiple HTTPS sites on one ALB Understood SSL policy selection for security compliance DNS Foundation:\nCreated Route 53 hosted zone and configured nameservers Set up A, CNAME, and Alias records for various services Understood TTL impact on DNS caching and propagation Configured private hosted zone for internal DNS resolution Advanced Routing:\nImplemented weighted routing for canary deployments (90/10 split) Configured latency-based routing for multi-region applications Set up failover routing with health checks for DR Tested geolocation routing for region-specific content Multi-VPC Architecture:\nEstablished VPC peering connections between multiple VPCs Updated route tables for cross-VPC communication Configured security groups to allow peered VPC traffic Tested inter-region VPC peering for global connectivity High Availability Design:\nDesigned multi-AZ architecture with ASG + ALB Implemented health checks for automatic instance replacement Reduced latency with cross-zone load balancing Achieved 99.9% uptime with automated failover "},{"uri":"https://khaicoderlor.github.io/intership-report-aws/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Building an EV Rental AI Agent with AWS Bedrock Overview EV Rental AI Agent is an intelligent chatbot built to assist customers in the VinFast electric vehicle rental system. This workshop demonstrates how to leverage AWS Bedrock, Claude 3.5 Sonnet, and Knowledge Bases to create a conversational AI that can:\nAnswer natural language questions in Vietnamese Automatically search information from multiple sources Display data as interactive cards in the chat interface Retrieve available vehicles and charging stations Access rental policies and FAQs from a knowledge base In this workshop, you will learn how to:\nSetup AWS Bedrock - Enable AI models and create a Knowledge Base for document retrieval Deploy Backend API - Build a FastAPI server with Strands Agent SDK for intelligent tool selection Deploy Frontend - Create a React chat interface with Chakra UI components Test the System - Interact with the AI agent and verify all functionalities Content Workshop Overview Prerequisites Setup AWS Bedrock Deploy Backend API Deploy Frontend Testing the AI Agent Clean Up Resources "},{"uri":"https://khaicoderlor.github.io/intership-report-aws/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my cloud engineering journey from 08/09/2025 to 01/12/2025, I had the opportunity to learn, apply, and strengthen a wide range of technical and professional skills in a structured, real-world cloud environment.\nThroughout this period, I participated in multiple hands-on activities such as:\nBuilding foundational AWS infrastructure (IAM, VPC, EC2, S3, EBS, EFS…) Designing high-availability architectures (ELB, ASG, RDS Multi-AZ, CloudFront) Working with managed databases (RDS, DynamoDB, Aurora) Implementing application modernization (ECS, Fargate, EKS, KOPS, Minikube) Building and optimizing CI/CD pipelines (Git Flow, Jenkins, CodePipeline) Implementing observability stack (Prometheus, Grafana, OpenTelemetry, Helios) Working with serverless architectures (Lambda, API Gateway, Step Functions) Building data and ML pipelines (Glue, Athena, QuickSight, SageMaker) Through these tasks, I improved significantly in cloud architecture, DevOps, automation, debugging, monitoring, communication, and overall engineering professionalism.\nI always strived to complete tasks with quality, maintained discipline, and collaborated effectively with teammates during all technical hands-on weeks.\nTo objectively reflect on my learning journey, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Applying cloud concepts in practice: VPC design, EC2, Load Balancing, Autoscaling, Kubernetes, Observability stack, DevOps pipelines ✅ ☐ ☐ 2 Ability to learn Ability to absorb new cloud technologies quickly (K8s, Jenkins, OTel, SageMaker, KOPS…) and apply them immediately in hands-on labs ✅ ☐ ☐ 3 Proactiveness Taking initiative in solving configuration issues, debugging deployments, optimizing infrastructure, and exploring advanced services ✅ ☐ ☐ 4 Sense of responsibility Completing daily worklogs, finishing every week\u0026rsquo;s tasks on time (Week 1 → Week 12), ensuring accuracy ✅ ☐ ☐ 5 Discipline Following study schedule (Mon–Fri), strictly documenting tasks, maintaining consistent learning habits ☐ ✅ ☐ 6 Progressive mindset Continuously improving through advanced tools such as Prometheus, Grafana, OTel, Helios, CDK, CI/CD ✅ ☐ ☐ 7 Communication Presenting task results clearly, summarizing complex topics (AWS networking, DevOps pipelines, Kubernetes workflows) ☐ ✅ ☐ 8 Teamwork Collaborating on shared architecture design, solving CI/CD and infrastructure problems with peers ☐ ✅ ☐ 9 Professional conduct Respecting workflow rules, version control standards (Git Flow), tagging conventions, and security best practices ✅ ☐ ☐ 10 Problem-solving skills Debugging Kubernetes deployments, fixing IAM permission issues, optimizing pipelines, troubleshooting networking problems ☐ ✅ ☐ 11 Contribution to project/team Providing consistent input for architecture design, proposing improvements to CI/CD flow, observability stack, and infrastructure ☐ ✅ ☐ 12 Overall Overall review of performance throughout 12 weeks ✅ ☐ ☐ Needs Improvement Improve discipline and consistency in documentation and reporting during weeks with heavy workloads Enhance communication clarity, especially when explaining complex architecture or troubleshooting steps to non-technical members Strengthen problem-solving depth for advanced debugging (Kubernetes networking, multi-cluster management, CI/CD failures) Summary Overall, the internship/study journey significantly improved my technical competencies in:\nCloud architecture (AWS) - VPC, EC2, ELB, ASG, Route 53, CloudFront DevOps tools - Git Flow, Jenkins, CodePipeline, CodeBuild, CodeDeploy Kubernetes ecosystem - Minikube, KOPS, Deployments, Services, Ingress Observability \u0026amp; Monitoring - Prometheus, Grafana, OpenTelemetry, Helios, CloudWatch CI/CD \u0026amp; automation - Jenkins pipelines, AWS native CI/CD, Infrastructure as Code Data \u0026amp; AI/ML services - Glue, Athena, QuickSight, SageMaker I have transitioned from foundational knowledge to practical, hands-on, production-level cloud engineering skills, preparing me for real enterprise environments and future certifications (AWS SAA, DVA, DevOps Engineer, CKA).\n"},{"uri":"https://khaicoderlor.github.io/intership-report-aws/5-workshop/5.6-testing/","title":"Testing the System","tags":[],"description":"","content":"Testing the EV Rental AI Agent In this section, you will test all three core features of the AI Agent to ensure everything works correctly.\nPrerequisites for Testing Before testing, ensure:\n✅ Backend server running on http://localhost:8000 ✅ Frontend application running on http://localhost:3000 ✅ PostgreSQL database is running and populated with test data ✅ AWS Bedrock Knowledge Base is synced and ready Test Scenario 1: Knowledge Base Search The AI Agent should be able to answer questions about rental policies, pricing, and FAQs using the Knowledge Base.\nTest Queries:\nRental Policy:\nUser: \u0026#34;Chính sách thuê xe là gì?\u0026#34; Expected: Agent returns rental policy details from Knowledge Base Required Documents:\nUser: \u0026#34;Tôi cần giấy tờ gì để thuê xe?\u0026#34; Expected: Agent lists required documents (ID, license, deposit info) Pricing Information:\nUser: \u0026#34;Giá thuê xe VinFast VF8 là bao nhiêu?\u0026#34; Expected: Agent provides pricing details from Knowledge Base Booking Process:\nUser: \u0026#34;Làm thế nào để đặt xe?\u0026#34; Expected: Agent explains step-by-step booking process Verification:\n✅ Response includes citation from Knowledge Base ✅ Answer is relevant and accurate ✅ Markdown formatting displays correctly ✅ Response time is under 5 seconds Test Scenario 2: Vehicle Search The AI Agent should search the PostgreSQL database for available vehicles based on location and date.\nTest Queries:\nSearch by Location:\nUser: \u0026#34;Tìm xe ở Hà Nội\u0026#34; Expected: Agent lists available vehicles in Hanoi Search by Model:\nUser: \u0026#34;Có xe VinFast VF8 nào available không?\u0026#34; Expected: Agent shows VF8 vehicles with availability status Search with Date Range:\nUser: \u0026#34;Tìm xe VF9 ở Hồ Chí Minh từ ngày 20/12 đến 25/12\u0026#34; Expected: Agent searches vehicles available in that date range Search with Price Range:\nUser: \u0026#34;Xe nào dưới 1 triệu đồng/ngày?\u0026#34; Expected: Agent filters vehicles by price Verification:\n✅ Agent correctly extracts search parameters (location, model, dates) ✅ Results include vehicle details (model, price, location, availability) ✅ Data is fetched from PostgreSQL database ✅ Results are formatted in a readable table or list Expected Response Format:\n## 🚗 Available Vehicles | Model | Location | Price/Day | Status | |-------|----------|-----------|--------| | VinFast VF8 | Hà Nội | 800,000đ | Available | | VinFast VF9 | Hà Nội | 1,200,000đ | Available | Test Scenario 3: Charging Station Finder The AI Agent should find nearby charging stations with real-time availability.\nTest Queries:\nSearch by District:\nUser: \u0026#34;Trạm sạc gần Quận Hoàn Kiếm\u0026#34; Expected: Agent lists charging stations in Hoan Kiem district Search by Address:\nUser: \u0026#34;Tìm trạm sạc ở Quận 1, TP.HCM\u0026#34; Expected: Agent finds stations in District 1, HCMC Check Station Availability:\nUser: \u0026#34;Trạm sạc nào còn trống?\u0026#34; Expected: Agent shows stations with available charging ports Filter by Connector Type:\nUser: \u0026#34;Trạm sạc có CCS2 connector\u0026#34; Expected: Agent filters stations with CCS2 connectors Verification:\n✅ Agent correctly identifies location from query ✅ Results include station name, address, and availability ✅ Connector types are listed ✅ Real-time availability status is shown Expected Response Format:\n## ⚡ Charging Stations Near You ### VinFast Station - Hoàn Kiếm 📍 Address: 123 Trần Hưng Đạo, Hoàn Kiếm, Hà Nội 🔌 Connectors: CCS2 (2 available), CHAdeMO (1 available) ⏰ Hours: 24/7 ✅ Status: Available Test Scenario 4: Multi-Turn Conversations Test the agent\u0026rsquo;s ability to maintain context across multiple turns.\nTest Conversation:\nUser: \u0026#34;Tôi muốn thuê xe VF8\u0026#34; Agent: [Provides VF8 information] User: \u0026#34;Giá bao nhiêu?\u0026#34; Agent: [Should understand context refers to VF8 pricing] User: \u0026#34;Trạm sạc gần đó ở đâu?\u0026#34; Agent: [Should find charging stations near VF8 location] Verification:\n✅ Agent maintains conversation context ✅ Pronouns and references are resolved correctly ✅ Session ID persists across messages Test Scenario 5: Error Handling Test how the agent handles invalid or unclear queries.\nTest Cases:\nAmbiguous Query:\nUser: \u0026#34;Xe\u0026#34; Expected: Agent asks for clarification Unavailable Vehicle:\nUser: \u0026#34;Tìm xe Tesla\u0026#34; Expected: Agent explains Tesla is not available, suggests alternatives Invalid Date:\nUser: \u0026#34;Thuê xe từ ngày 32/13\u0026#34; Expected: Agent detects invalid date and asks for correction Out of Scope:\nUser: \u0026#34;What\u0026#39;s the weather today?\u0026#34; Expected: Agent politely explains it can only help with EV rentals Verification:\n✅ Agent handles errors gracefully ✅ Provides helpful error messages ✅ Suggests alternatives when possible Performance Testing Check system performance under normal usage:\nMetrics to Monitor:\nResponse Time:\nKnowledge Base queries: \u0026lt; 3 seconds Vehicle search: \u0026lt; 2 seconds Charging station search: \u0026lt; 2 seconds API Health:\ncurl http://localhost:8000/health Expected: 200 OK with health status\nBackend Logs: Check for errors in FastAPI console output\nFrontend Console: Open browser DevTools → Console\nNo JavaScript errors API calls succeed (Network tab) Integration Testing Checklist Run through this comprehensive checklist:\n✅ Knowledge Base Integration:\nAgent can retrieve policy information Citations are included in responses Bedrock API calls succeed ✅ Database Integration:\nVehicle search queries PostgreSQL Results are accurate and up-to-date Database connection is stable ✅ Backend API:\n/api/chat endpoint works /health endpoint responds Session management functions correctly ✅ Frontend UI:\nMessages display correctly User input is captured Loading states work Markdown renders properly Auto-scroll functions ✅ Error Handling:\nNetwork errors are caught Invalid inputs handled gracefully User receives helpful feedback Testing with Postman (Optional) Test backend API directly:\n1. Health Check:\nGET http://localhost:8000/health 2. Chat Request:\nPOST http://localhost:8000/api/chat Content-Type: application/json { \u0026#34;session_id\u0026#34;: \u0026#34;test-session-123\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Chính sách thuê xe là gì?\u0026#34; } Expected Response:\n{ \u0026#34;response\u0026#34;: \u0026#34;## 📋 Chính sách thuê xe VinFast\\n\\n...\u0026#34;, \u0026#34;data\u0026#34;: null, \u0026#34;session_id\u0026#34;: \u0026#34;test-session-123\u0026#34; } Troubleshooting Test Failures Issue: Knowledge Base returns empty results\nCheck Knowledge Base is synced in AWS Console Verify KNOWLEDGE_BASE_ID in .env Test KB directly in Bedrock console Issue: Vehicle search returns no results\nCheck PostgreSQL database has test data Verify DATABASE_URL connection string Run SQL query directly: SELECT * FROM vehicles; Issue: Charging stations not found\nVerify backend API /stations endpoint works Check station data in database Test API call: curl http://localhost:8080/stations Issue: Frontend not connecting to backend\nCheck REACT_APP_API_URL in frontend .env Verify backend CORS allows http://localhost:3000 Check browser console for network errors Test Report Template Document your test results:\n## Test Report - EV Rental AI Agent **Date:** 2024-12-20 **Tester:** Your Name ### Test Results Summary - Total Tests: 15 - Passed: 14 - Failed: 1 - Success Rate: 93% ### Detailed Results #### Knowledge Base Search - [x] Rental policy query - PASS - [x] Required documents - PASS - [x] Pricing information - PASS - [ ] Booking process - FAIL (slow response) #### Vehicle Search - [x] Search by location - PASS - [x] Search by model - PASS - [x] Date range search - PASS #### Charging Station Finder - [x] District search - PASS - [x] Availability check - PASS ### Issues Found 1. Booking process query takes 7 seconds (\u0026gt; 5s threshold) - Root cause: Knowledge Base sync incomplete - Fix: Re-sync data source ### Recommendations - Monitor response times during peak usage - Add caching for frequently asked questions - Implement rate limiting Success! 🎉 Your EV Rental AI Agent is now fully tested and operational.\nNext: Proceed to Cleanup to remove resources and avoid charges.\n"},{"uri":"https://khaicoderlor.github.io/intership-report-aws/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Master CloudFront CDN for global content delivery Learn Lambda@Edge for edge computing Understand Docker Compose for multi-container applications Practice Docker Swarm for container orchestration Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Mon CloudFront CDN: - CloudFront architecture and edge locations - Origin types: S3, ALB, custom origins - Cache behaviors and TTL settings Practice: Deploy CloudFront distribution with S3 origin 13/10/2025 13/10/2025 https://cloudjourney.awsstudygroup.com/ Tue CloudFront Advanced: - Origin groups for high availability - Field-level encryption - Geo-restriction and signed URLs/cookies Practice: Configure cache invalidation, custom error pages 14/10/2025 14/10/2025 https://cloudjourney.awsstudygroup.com/ Wed Lambda@Edge: - Lambda@Edge use cases (viewer/origin request/response) - Modifying headers and redirects at edge - A/B testing with Lambda@Edge Practice: Deploy Lambda@Edge for dynamic content modification 15/10/2025 15/10/2025 https://cloudjourney.awsstudygroup.com/ Thu Docker Compose: - docker-compose.yml syntax and structure - Multi-container applications (web + db + cache) - Docker networks and volumes in compose Practice: Create compose file for microservices stack 16/10/2025 16/10/2025 https://cloudjourney.awsstudygroup.com/ Fri Docker Swarm: - Swarm mode vs standalone Docker - Services, stacks, and overlay networks - Docker secrets and configs Practice: Initialize swarm cluster, deploy replicated services 17/10/2025 17/10/2025 https://cloudjourney.awsstudygroup.com/ Week 6 Achievements: CloudFront CDN Mastery:\nDeployed CloudFront distribution with S3 and ALB origins Configured multiple cache behaviors for different URL patterns Implemented geo-restriction for content access control Reduced global latency by 60% using edge locations CloudFront Advanced Features:\nSet up origin groups for automatic failover Configured field-level encryption for sensitive data Implemented signed URLs and cookies for private content Practiced cache invalidation strategies Edge Computing:\nDeployed Lambda@Edge functions for viewer request modification Implemented A/B testing with Lambda@Edge header manipulation Modified response headers for security (HSTS, CSP) Understood CloudFront function vs Lambda@Edge differences Docker Compose Proficiency:\nCreated docker-compose.yml for 3-tier application (web, API, db, Redis) Configured service dependencies and health checks Implemented Docker networks for service isolation Practiced volume mounts for data persistence Container Orchestration:\nInitialized Docker Swarm cluster with manager and worker nodes Deployed replicated services across swarm Configured overlay networks for multi-host communication Implemented rolling updates with zero downtime Performance \u0026amp; Security:\nAchieved 70% bandwidth cost reduction with CloudFront caching Implemented custom SSL certificates for CloudFront Used Docker secrets for sensitive configuration Optimized cache hit ratio to 85% "},{"uri":"https://khaicoderlor.github.io/intership-report-aws/5-workshop/5.7-cleanup/","title":"Cleanup Resources","tags":[],"description":"","content":"Cleaning Up Resources After completing the workshop, follow these steps to clean up all resources and avoid unnecessary AWS charges.\nWhy Cleanup is Important Cost Savings: AWS charges for active resources like Bedrock Knowledge Bases, S3 storage, and running services Security: Remove unused IAM credentials to maintain security best practices Organization: Keep your AWS account clean and organized Step 1: Delete AWS Bedrock Knowledge Base 1.1 Delete Knowledge Base Open the AWS Bedrock Console Navigate to Knowledge Bases in the left sidebar Select your Knowledge Base: ev-rental-knowledge-base Click Delete Confirm deletion by typing the Knowledge Base name Click Delete to confirm ⚠️ Note: This will also delete the associated data source connections.\n1.2 Delete S3 Bucket and Documents Open the S3 Console Find your bucket: ev-rental-knowledge-docs Select the bucket Click Empty to delete all objects Confirm by typing \u0026ldquo;permanently delete\u0026rdquo; After emptying, click Delete on the bucket Confirm by typing the bucket name Or use AWS CLI:\n# Delete all objects in bucket aws s3 rm s3://ev-rental-knowledge-docs --recursive # Delete the bucket aws s3 rb s3://ev-rental-knowledge-docs Step 2: Delete IAM User and Access Keys 2.1 Delete Access Keys Open the IAM Console Navigate to Users Select your user (e.g., bedrock-agent-user) Click on the Security credentials tab Under Access keys, find your access key Click Delete next to the access key Confirm deletion 2.2 Delete IAM User (Optional) If you created a dedicated IAM user for this workshop:\nIn the IAM Console, select the user Click Delete user Confirm by checking the box Click Delete Or use AWS CLI:\n# List access keys aws iam list-access-keys --user-name bedrock-agent-user # Delete access key (replace with your key ID) aws iam delete-access-key --user-name bedrock-agent-user --access-key-id AKIA5GPEMGJZK6E7PMEB # Delete user aws iam delete-user --user-name bedrock-agent-user Step 3: Stop Local Services 3.1 Stop FastAPI Backend In the terminal where FastAPI is running:\nPress Ctrl + C to stop the server\nDeactivate virtual environment:\ndeactivate Optionally delete the project folder:\n# On macOS/Linux rm -rf ev-rental-backend # On Windows rmdir /s ev-rental-backend 3.2 Stop React Frontend In the terminal where React is running:\nPress Ctrl + C to stop the development server\nOptionally delete the project folder:\n# On macOS/Linux rm -rf ev-rental-frontend # On Windows rmdir /s ev-rental-frontend 3.3 Stop PostgreSQL Database If you installed PostgreSQL locally for this workshop:\nOn macOS:\n# Stop PostgreSQL service brew services stop postgresql@14 On Linux:\nsudo systemctl stop postgresql On Windows:\n# Open Services (services.msc) # Find \u0026#34;PostgreSQL\u0026#34; service # Right-click → Stop 3.4 Delete Database (Optional) If you want to completely remove the database:\n# Connect to PostgreSQL psql -U postgres # Drop database DROP DATABASE ev_rental_db; # Exit \\q Step 4: Remove Environment Files Delete sensitive .env files that contain credentials:\nBackend:\ncd ev-rental-backend rm .env Frontend:\ncd ev-rental-frontend rm .env ⚠️ Security Note: Never commit .env files to Git. Always add them to .gitignore.\nStep 5: Verify Cleanup 5.1 Check AWS Resources Verify all resources are deleted:\nBedrock Console:\nNo Knowledge Bases listed No model invocations active S3 Console:\nBucket ev-rental-knowledge-docs is deleted IAM Console:\nAccess keys are deleted IAM user removed (if you chose to delete it) 5.2 Check AWS Costs Open the AWS Billing Console Check Bills for the current month Verify charges: Bedrock charges should stop after Knowledge Base deletion S3 storage charges should stop after bucket deletion No ongoing compute charges Or use AWS CLI:\naws ce get-cost-and-usage \\ --time-period Start=2024-12-01,End=2024-12-31 \\ --granularity MONTHLY \\ --metrics UnblendedCost \\ --group-by Type=SERVICE Cost Breakdown Here\u0026rsquo;s what you may have been charged during the workshop:\nService Estimated Cost Notes AWS Bedrock - Claude 3.5 Sonnet ~$0.50 - $2.00 Depends on number of queries AWS Bedrock - Knowledge Base ~$0.10 - $0.50 Vector storage and retrieval S3 Storage ~$0.02 Minimal for small documents Data Transfer ~$0.05 Usually within free tier Total ~$0.67 - $2.57 Approximate for workshop ⚠️ Note: Most costs come from Bedrock API calls. The longer you test, the higher the cost.\nCleanup Checklist Before you finish, verify all items are complete:\nAWS Resources ✅ Bedrock Knowledge Base deleted ✅ S3 bucket emptied and deleted ✅ IAM Access Keys deleted ✅ IAM User deleted (optional) Local Resources ✅ FastAPI backend stopped ✅ React frontend stopped ✅ PostgreSQL database stopped ✅ PostgreSQL database dropped (optional) Sensitive Files ✅ Backend .env file deleted ✅ Frontend .env file deleted ✅ No AWS credentials in project files Verification ✅ AWS Console shows no active resources ✅ Billing dashboard shows stopped charges ✅ Local services not running Troubleshooting Cleanup Issues Issue: Cannot delete S3 bucket - \u0026ldquo;Bucket not empty\u0026rdquo;\nSolution: Empty all objects first using the S3 Console or CLI Command: aws s3 rm s3://bucket-name --recursive Issue: Cannot delete Knowledge Base - \u0026ldquo;In use\u0026rdquo;\nSolution: Wait a few minutes for any pending operations to complete Check if any API calls are still referencing it Issue: IAM User deletion fails - \u0026ldquo;User has attached policies\u0026rdquo;\nSolution: Detach all policies first Go to IAM → Users → Select user → Permissions → Detach policies Issue: PostgreSQL won\u0026rsquo;t stop\nSolution: Force kill the process On macOS/Linux: sudo killall postgres On Windows: Use Task Manager to end PostgreSQL processes Optional: Keep Learning If you want to continue experimenting:\nKeep These Resources: ✅ IAM User (with minimal permissions) ✅ Bedrock model access (no charge when not in use) What You Can Do Next: Add more documents to Knowledge Base Implement additional agent tools Deploy to AWS Lambda for serverless operation Add authentication and user management Integrate with real vehicle booking systems Conclusion 🎉 Congratulations! You have successfully:\n✅ Built an AI Agent using AWS Bedrock and Claude 3.5 Sonnet ✅ Integrated Knowledge Bases for intelligent document retrieval ✅ Created a FastAPI backend with Strands Agent SDK ✅ Developed a React frontend for user interaction ✅ Tested all features end-to-end ✅ Cleaned up resources to avoid charges Key Takeaways: AI Agents can autonomously select tools and make decisions AWS Bedrock simplifies access to foundation models like Claude Knowledge Bases enable semantic search over documents Strands SDK provides a framework for building agent workflows FastAPI + React create a modern full-stack AI application Next Steps: Explore other Bedrock models (Llama 3, Mistral, etc.) Learn about RAG (Retrieval Augmented Generation) Build more complex agent workflows Deploy to production using AWS services "},{"uri":"https://khaicoderlor.github.io/intership-report-aws/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":"Feedback on FCJ Workforce Program Experience Throughout the 12-week FCJ Workforce program, I have gained invaluable experience in a professional work environment, continuous learning culture, and practical Cloud Computing skill development opportunities.\n1. Overall Program Evaluation Outstanding Strengths\nProfessional and Friendly Work Environment\nThe work environment at FCJ is both professional and welcoming, providing the best conditions for intern development. The learning through practice culture is applied throughout, helping me not only learn theory but also practice directly on real projects. The workspace is modern, fully equipped with necessary AWS tools and resources.\nActive Learning Community\nParticipating in the AWS Cloud Clubs Vietnam community is one of the program\u0026rsquo;s greatest strengths. I had opportunities to connect with other interns, exchange experiences, and learn from each other. Weekly knowledge sharing sessions are very beneficial.\n2. Support from Mentors and Admin Team Mentors\nMy mentor has deep AWS expertise and rich practical experience. What I appreciate most:\nEffective Guidance Approach: Instead of providing immediate answers, my mentor encourages me to research, debug, and find solutions independently. When facing difficulties, they guide me on how to approach problems rather than solving them for me.\nReal-world Experience Sharing: My mentor frequently shares case studies from actual projects, helping me understand how to apply knowledge in production environments.\nAdmin Team\nThe admin team provides excellent support for:\nOrganizing workshops, seminars, and networking events Answering questions about company processes and policies Flexible time support when balancing work with studies 3. Alignment with Major and Career Goals Academic Alignment\nWith a Computer Science or Information Technology background, the FCJ program perfectly aligns with what I learned:\nKnowledge of networking, operating systems, and databases is directly applied to AWS services Programming skills are used to build serverless applications and automation scripts Knowledge of data structures and algorithms helps me optimize cloud architecture New Skills Expansion\nThe program not only reinforces foundational knowledge but also exposes me to many new technologies:\nCloud-native technologies: Docker, Kubernetes, ECS, EKS Infrastructure as Code: CloudFormation, Terraform, AWS CDK DevOps practices: CI/CD pipelines, observability Generative AI and machine learning: Amazon Bedrock, SageMaker, RAG architecture Security best practices: IAM, GuardDuty, Security Hub, compliance frameworks 4. Learning and Skill Development Opportunities Technical Skills\nAWS Services Mastery\nOver 12 weeks, I gained hands-on experience with over 30 different AWS services, from compute (EC2, Lambda), storage (S3, EFS), databases (RDS, DynamoDB), to AI and machine learning services (Bedrock, SageMaker).\nCloud Architecture Design\nLearned to design cloud architecture following the AWS Well-Architected Framework with 6 pillars: Operational Excellence, Security, Reliability, Performance Efficiency, Cost Optimization, and Sustainability.\nDevOps and Automation\nPracticed building CI/CD pipelines with AWS CodePipeline, CodeBuild, CodeDeploy, and automating infrastructure provisioning with Infrastructure as Code tools.\nSecurity and Compliance\nMastered best practices in cloud security, identity management, data protection, and compliance standards.\nSoft Skills\nCommunication and Collaboration\nLearned to work effectively in teams with Git workflow (branching, pull requests, code reviews) Technical presentations during demo sessions and knowledge sharing Writing clear technical documentation for projects Problem-solving and Critical Thinking\nDeveloped systematic problem analysis thinking Learned to debug and troubleshoot production issues Applied root cause analysis to resolve incidents Time Management and Self-learning\nManaged time effectively between multiple tasks and deadlines Self-researched and learned new technologies proactively Built continuous learning habits Professional Communication\nWriting professional emails and reports Participating in meetings and contributing ideas effectively Networking with industry professionals 5. Company Culture and Team Spirit FCJ Culture\nLearning Culture\nContinuous learning culture is strongly encouraged. Everyone is willing to share knowledge, exchange experiences, and support each other. No question is too basic or shouldn\u0026rsquo;t be asked.\nInnovation and Experimentation\nInterns are encouraged to experiment, innovate, and not fear failure. The fail fast, learn faster culture helps me dare to try new approaches without fear of judgment.\nTransparency and Open Communication\nInformation is shared transparently, from project roadmaps to technical decisions. Everyone can contribute ideas and feedback is taken seriously.\nTeam Spirit\nCollaborative Environment\nAlthough an intern, I am treated like a full team member. My opinions are respected and influence technical decisions.\nSupport System\nWhen facing difficulties or tight deadlines, everyone is ready to help regardless of position or role. Having a solid support network makes me always feel secure.\nCelebrating Success\nEvery milestone and achievement is celebrated, creating motivation to work harder and contribute more.\n6. What I\u0026rsquo;m Most Satisfied With Top Satisfactions\n1. Hands-on Experience with Real Projects\nWorked on actual projects, not just simulation exercises. I contributed to production codebase and saw the impact of my work on end users.\n2. AWS Certifications Journey\nThe program helped me have a clear roadmap to pursue AWS certifications.\n3. Mentorship Quality\nMentorship quality exceeded expectations. My mentor not only teaches technical skills but also shares career advice, industry insights, and soft skills development.\n4. Community and Networking\nConnected with a broad AWS community, from fellow interns to senior engineers and AWS Heroes. This network is invaluable for career development.\n5. Comprehensive Learning Path\nThe learning roadmap covers everything from fundamentals to advanced topics, from technical skills to soft skills. Feels like a well-invested and comprehensive approach.\n6. Work-life Balance\nAlthough the workload can be demanding, the company always respects work-life balance. Flexible with time and no pressure to work overtime.\n7. Improvement Suggestions for Future Interns Technical Improvements\n1. Pre-program Preparation\nProvide pre-reading materials or prerequisite courses before starting the program Self-assessment test so interns know their current level and focus areas Detailed setup guide for development environment 2. Structured Code Review Process\nHave specific checklist for code reviews to ensure consistency Organize code review sessions so everyone learns from each other\u0026rsquo;s code Recording or written summary of review comments for future reference 3. More Advanced Workshops\nAdd workshops on advanced topics like multi-region architecture, disaster recovery, cost optimization strategies Hands-on labs on security best practices and incident response Workshops on soft skills: presentation, technical writing, negotiation 4. Mock Scenarios\nOrganize mock incident response drills Simulation of production issues to practice troubleshooting Mock architecture design sessions for AWS certifications prep Process and Communication\n5. Onboarding Enhancement\nHave buddy program so new interns have peer mentors besides official mentors First-week orientation on tools, processes, and company culture Clear documentation on who to contact for different types of issues 6. Regular Feedback Loop\nBi-weekly one-on-ones with mentor instead of monthly Mid-program review to adjust learning path if needed Anonymous feedback channel for interns to comfortably share concerns 7. Knowledge Sharing Platform\nInternal wiki or knowledge base to document learnings Platform for interns to share tips, tricks, and solutions Archive of past projects to learn from previous interns Career Development\n8. Career Path Guidance\nWorkshop on different career paths in cloud industry Connect with alumni to understand career progression Clear criteria for internship-to-full-time conversion (if available) 9. Portfolio Building Support\nGuidance on how to build and showcase portfolio projects Help with resume and LinkedIn optimization Mock interview sessions to prepare for job hunting 8. Recommendation to Friends I would definitely recommend FCJ Workforce!\nReasons:\n1. High-Quality Training\nThe program is systematically designed with curriculum up-to-date with industry trends. Not a tea-making internship but truly learning valuable skills.\n2. Real-world Experience\nWorking with production systems, actual AWS services, and professional development workflow. This experience is very valuable for resumes.\n3. Networking Value\nConnecting with AWS community, industry professionals, and potential employers. This network is very useful for career development.\n4. Career Boost\nHaving AWS certifications, real project experience, and strong recommendation letters will make resumes stand out much more compared to peers with only academic background.\n5. Supportive Environment\nMentors and team always provide support, culture encourages learning. This is a good environment to grow both technical and soft skills.\nSuitable Candidates:\nJunior or senior students wanting experience before graduation Those passionate about Cloud Computing and wanting to pursue careers in this field Anyone planning to take AWS certifications Those wanting to transition to cloud engineering roles Advice for Friends:\nPrepare foundational knowledge in networking, Linux, and programming before applying Don\u0026rsquo;t hesitate to ask questions and maximize mentorship opportunities Be proactive in exploring and researching Balance workload and rest well to sustain energy Network with people and build relationships 9. Future Aspirations and Plans Short-term Goals (3-6 months)\nAWS Certifications\nComplete AWS Solutions Architect Associate exam Start preparing for Developer Associate or SysOps Administrator Associate Contribute to AWS community through blog posts and workshops Technical Skills\nDeep dive into Kubernetes and container orchestration Learn more about FinOps and cloud cost optimization Explore more about Generative AI applications on AWS Portfolio Development\nBuild 2-3 showcase projects demonstrating AWS skills Document learnings through technical blog Contribute to open-source projects related to AWS Long-term Vision (1-2 years)\nCareer Direction\nPursue Cloud Engineer or DevOps Engineer role Aim for Solutions Architect position in the future Consider specialization in Security or AI and machine learning Continuous Learning\nPursue advanced AWS certifications (Professional level) Learn multiple cloud platforms (Azure, GCP) for broader perspective Develop leadership and project management skills Community Contribution\nActive participation in AWS Cloud Clubs Mentoring future interns or junior developers Speaking at technical conferences and events About Continuing with FCJ\nWould love the opportunity to:\nWork part-time during final year to maintain connection and continue learning If opportunity arises, I would love to join full-time after graduation Join alumni network to stay connected and contribute back In the future, I want to become a mentor for new interns 10. Acknowledgments I sincerely thank:\n\u0026ldquo;Reflecting on my journey at FCJ, I realize that my growth is deeply rooted in the dedicated guidance of the Admin and Mentor team. I would like to express my profound gratitude to Mr. Nguyen Gia Hung, Mr. Van Hoang Kha, Mr. Lu Hoan Thien, Mr. Nguyen Tuan Thinh, Mr. Le Nguyen Vu Hoang, and Mr. Nguyen Dong Thanh Hiep.\nYour meticulous instruction—ranging from the smallest technical details to a strategic approach to learning—has helped me build a solid foundation. I deeply cherish the patience and timely encouragement you extended to me during challenging moments. These invaluable lessons will serve as essential preparation for my future career path. I wish you all continued success in life and hope you keep the flame of passion alive to help the AWS community grow even stronger.\nThe past 12 weeks with FCJ Workforce have been one of the most valuable experiences of my educational journey. The knowledge, skills, and connections I have gained will serve as a firm foundation for my future career.\nThank you, FCJ, for giving me this opportunity!\n"},{"uri":"https://khaicoderlor.github.io/intership-report-aws/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Master API Gateway for RESTful API management Learn Lambda advanced features and patterns Understand DynamoDB NoSQL database Practice Step Functions and EventBridge for workflows Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Mon API Gateway REST API: - API Gateway stages, deployments, and versioning - Request/response transformations - API keys, usage plans, and throttling Practice: Create REST API with Lambda integration 20/10/2025 20/10/2025 https://cloudjourney.awsstudygroup.com/ Tue Lambda Advanced: - Lambda layers for code reusability - Environment variables and secrets integration - Lambda destinations and dead-letter queues Practice: Build Lambda with layers, implement error handling 21/10/2025 21/10/2025 https://cloudjourney.awsstudygroup.com/ Wed DynamoDB Basics: - DynamoDB tables, partition keys, sort keys - Read/write capacity modes: provisioned vs on-demand - DynamoDB Streams for change data capture Practice: Create table, perform CRUD operations, query patterns 22/10/2025 22/10/2025 https://cloudjourney.awsstudygroup.com/ Thu AWS Step Functions: - State machine design with ASL (Amazon States Language) - Task states, choice states, parallel execution - Error handling and retry policies Practice: Create workflow orchestrating Lambda functions 23/10/2025 23/10/2025 https://cloudjourney.awsstudygroup.com/ Fri EventBridge: - Event-driven architecture patterns - Event buses, rules, and targets - Schedule expressions for cron jobs Practice: Create event rules triggering Lambda/Step Functions 24/10/2025 24/10/2025 https://cloudjourney.awsstudygroup.com/ Week 7 Achievements: API Gateway Mastery:\nCreated REST API with multiple resources and methods Configured request/response mapping templates Implemented API key authentication and usage plans Set up throttling and rate limiting for API protection Lambda Advanced Skills:\nCreated Lambda layers for shared code/dependencies Configured environment variables and Secrets Manager integration Implemented destinations for async invocations Set up DLQ (Dead Letter Queue) for failed executions DynamoDB Proficiency:\nDesigned DynamoDB table with optimal partition/sort key Implemented efficient query and scan patterns Configured DynamoDB Streams for change tracking Understood single-table design patterns Workflow Orchestration:\nCreated Step Functions state machine with ASL Implemented parallel execution and error handling Configured retry policies and exponential backoff Orchestrated multi-Lambda workflows successfully Event-Driven Architecture:\nBuilt EventBridge rules for real-time event routing Created custom event buses for application events Scheduled Lambda executions with cron expressions Implemented event filtering with content-based routing Serverless Integration:\nBuilt complete serverless API: API Gateway + Lambda + DynamoDB Reduced infrastructure management overhead to zero Achieved auto-scaling without configuration Understood serverless pricing and cost optimization "},{"uri":"https://khaicoderlor.github.io/intership-report-aws/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Master CloudWatch for monitoring and logging Understand CloudTrail for auditing and compliance Learn X-Ray for distributed tracing Practice AWS Config and cost optimization strategies Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Mon CloudWatch Metrics \u0026amp; Alarms: - CloudWatch metrics, custom metrics, and dashboards - Alarm states and SNS notifications - Metric math and composite alarms Practice: Create dashboard, set up alarms for EC2/RDS/Lambda 27/10/2025 27/10/2025 https://cloudjourney.awsstudygroup.com/ Tue CloudWatch Logs \u0026amp; Insights: - Log groups, streams, and retention policies - CloudWatch Logs Insights query syntax - Metric filters from log data Practice: Centralize logs from multiple sources, run queries 28/10/2025 28/10/2025 https://cloudjourney.awsstudygroup.com/ Wed CloudTrail \u0026amp; X-Ray: - CloudTrail for API call auditing - X-Ray service map and trace analysis - X-Ray SDK instrumentation Practice: Enable CloudTrail, instrument Lambda with X-Ray 29/10/2025 29/10/2025 https://cloudjourney.awsstudygroup.com/ Thu AWS Config \u0026amp; Compliance: - Config rules for resource compliance - Config remediation actions - Conformance packs for multi-account governance Practice: Create config rules for security compliance 30/10/2025 30/10/2025 https://cloudjourney.awsstudygroup.com/ Fri Cost Optimization: - Cost Explorer and budgets - Reserved Instances vs Savings Plans - Trusted Advisor recommendations Practice: Analyze costs, set budgets, implement tagging strategy 31/10/2025 31/10/2025 https://cloudjourney.awsstudygroup.com/ Week 8 Achievements: Monitoring Expertise:\nCreated comprehensive CloudWatch dashboards for all services Set up alarms with SNS notifications for critical metrics Implemented custom metrics for application-level monitoring Configured composite alarms for complex alerting logic Log Management:\nCentralized logs from EC2, Lambda, and containers to CloudWatch Logs Created metric filters to extract KPIs from log data Used CloudWatch Logs Insights for troubleshooting Set up log retention policies for cost optimization Tracing \u0026amp; Auditing:\nEnabled CloudTrail for all API activity auditing Instrumented Lambda functions with X-Ray SDK Analyzed service maps to identify performance bottlenecks Traced request flow across distributed microservices Compliance \u0026amp; Governance:\nConfigured AWS Config rules for security compliance Set up automatic remediation for non-compliant resources Implemented conformance packs for CIS benchmarks Tracked configuration changes over time Cost Optimization:\nAnalyzed spending patterns with Cost Explorer Created budgets with threshold alerts Identified opportunities for Reserved Instances (30% savings) Implemented resource tagging for cost allocation Operational Excellence:\nReduced MTTR (Mean Time To Resolution) by 50% with better monitoring Implemented proactive alerting before service degradation Achieved 99.9% uptime with health checks and auto-recovery Optimized costs by 25% through rightsizing recommendations "},{"uri":"https://khaicoderlor.github.io/intership-report-aws/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Master Cognito for authentication and authorization Learn Secrets Manager and KMS for secrets management Understand WAF and Shield for security Review and practice hands-on labs Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Mon AWS Cognito: - User pools vs identity pools - OAuth2 flows and JWT tokens - Cognito Hosted UI customization Practice: Implement authentication flow with Cognito 03/11/2025 03/11/2025 https://cloudjourney.awsstudygroup.com/ Tue Secrets Manager \u0026amp; Parameter Store: - Secrets Manager for database credentials rotation - Systems Manager Parameter Store for configuration - Integration with Lambda and ECS Practice: Store and rotate database secrets 04/11/2025 04/11/2025 https://cloudjourney.awsstudygroup.com/ Wed AWS KMS (Key Management Service): - Customer managed keys vs AWS managed keys - Envelope encryption and data keys - Key policies and grants Practice: Encrypt S3 objects and EBS volumes with KMS 05/11/2025 05/11/2025 https://cloudjourney.awsstudygroup.com/ Thu WAF \u0026amp; Shield: - AWS WAF rules and web ACLs - Shield Standard vs Shield Advanced - DDoS protection strategies Practice: Create WAF rules to block SQL injection 06/11/2025 06/11/2025 https://cloudjourney.awsstudygroup.com/ Fri Review Labs: - Hands-on review: S3, RDS, Lambda, API Gateway - Practice troubleshooting common issues - Prepare knowledge map of all services learned Practice: Complete comprehensive integration lab 07/11/2025 07/11/2025 https://cloudjourney.awsstudygroup.com/ Week 9 Achievements: Authentication Mastery:\nImplemented Cognito user pools with MFA Configured OAuth2 flows for web and mobile apps Customized Cognito Hosted UI with branding Integrated Cognito with API Gateway for authorization Secrets Management:\nConfigured automatic rotation for RDS credentials Stored application secrets in Secrets Manager Used Parameter Store for hierarchical configuration Integrated secrets with Lambda and ECS tasks Encryption \u0026amp; Key Management:\nCreated customer managed keys in KMS Implemented envelope encryption for large data Configured automatic key rotation Encrypted S3 buckets and EBS volumes with KMS Security \u0026amp; DDoS Protection:\nCreated WAF web ACLs with rate limiting rules Implemented geo-blocking and IP whitelisting Configured managed rules for OWASP Top 10 Understood Shield Standard vs Advanced benefits Comprehensive Review:\nCompleted integration lab combining 10+ AWS services Troubleshot common connectivity and permission issues Created knowledge map visualizing service relationships Documented best practices and lessons learned Security Posture:\nImplemented defense-in-depth security strategy Achieved least privilege access with IAM policies Encrypted data at rest and in transit Configured comprehensive security monitoring "},{"uri":"https://khaicoderlor.github.io/intership-report-aws/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Master Jenkins advanced features and pipelines Practice Git Flow branching strategy Learn AWS CodePipeline for CI/CD Understand CodeBuild and CodeDeploy for automation Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Mon Jenkins Advanced: - Declarative pipelines with Jenkinsfile - Pipeline stages, steps, and parallel execution - Jenkins plugins: Git, Docker, AWS Practice: Create multi-stage pipeline for build, test, deploy 10/11/2025 10/11/2025 https://cloudjourney.awsstudygroup.com/ Tue Git Flow Practice: - Feature branches, release branches, hotfix - Merge strategies and conflict resolution - Git Flow with Jenkins integration Practice: Implement Git Flow workflow for project 11/11/2025 11/11/2025 https://cloudjourney.awsstudygroup.com/ Wed AWS CodePipeline: - Pipeline structure: source, build, test, deploy stages - Integration with GitHub, CodeCommit - Manual approval steps Practice: Create CodePipeline for automated deployments 12/11/2025 12/11/2025 https://cloudjourney.awsstudygroup.com/ Thu AWS CodeBuild: - buildspec.yml configuration - Build environments and Docker images - Artifact management with S3 Practice: Configure CodeBuild for Docker image builds 13/11/2025 13/11/2025 https://cloudjourney.awsstudygroup.com/ Fri AWS CodeDeploy: - Deployment groups and strategies (rolling, blue/green) - appspec.yml for deployment configuration - CodeDeploy hooks and lifecycle events Practice: Deploy to EC2 and ECS with CodeDeploy 14/11/2025 14/11/2025 https://cloudjourney.awsstudygroup.com/ Week 10 Achievements: Jenkins Pipeline Mastery:\nCreated declarative Jenkinsfile with multi-stage pipeline Implemented parallel test execution for faster builds Configured Jenkins agents for distributed builds Integrated Docker and AWS plugins Git Flow Proficiency:\nImplemented feature branch workflow for development Practiced release branches for version management Handled hotfix branches for production issues Mastered merge conflict resolution strategies CI/CD Pipeline Automation:\nBuilt end-to-end CodePipeline: GitHub → CodeBuild → CodeDeploy Configured manual approval gates for production Implemented pipeline notifications with SNS Achieved automated deployments on every commit Build Automation:\nCreated buildspec.yml for Docker image builds Configured build caching for faster compile times Pushed artifacts to ECR and S3 Implemented build matrix for multi-environment testing Deployment Strategies:\nConfigured blue/green deployments with CodeDeploy Implemented rolling deployments with minimal downtime Created appspec.yml with lifecycle hooks Tested automatic rollback on deployment failure DevOps Excellence:\nReduced deployment time from 2 hours to 10 minutes Achieved 95% automation of deployment process Implemented continuous delivery for rapid iteration Established GitOps workflow with infrastructure as code "},{"uri":"https://khaicoderlor.github.io/intership-report-aws/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Master Kubernetes container orchestration Learn monitoring with Prometheus and Grafana Understand OpenTelemetry (OTeL) for observability Deploy applications to AWS with KOPS Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Mon Kubernetes Fundamentals: - Kubernetes architecture: master \u0026amp; worker nodes - Pods, Deployments, Services, ConfigMaps, Secrets - Minikube installation and setup Practice: Install Minikube, create first deployment 17/11/2025 17/11/2025 https://kubernetes.io/ Tue Kubernetes Advanced: - Namespaces and resource quotas - Persistent Volumes and Storage Classes - Ingress controllers Practice: Deploy multi-tier app with database and volume 18/11/2025 18/11/2025 https://kubernetes.io/ Wed KOPS \u0026amp; AWS Deployment: - KOPS (Kubernetes Operations) introduction - Create production-ready K8s cluster on AWS - Deploy demo application to KOPS cluster Practice: Deploy sample microservices app on AWS K8s 19/11/2025 19/11/2025 https://kops.sigs.k8s.io/ Thu Prometheus \u0026amp; Grafana: - Prometheus metrics collection and PromQL - Grafana dashboard creation - Alerting rules and notification channels Practice: Monitor K8s cluster, create custom dashboards 20/11/2025 20/11/2025 https://prometheus.io/ Fri OpenTelemetry \u0026amp; Observability: - OTeL for traces, metrics, logs - Instrument application with OTeL - Helios project: data \u0026amp; metrics collection for performance monitoring Practice: Add OTeL to demo app, visualize in Grafana 21/11/2025 21/11/2025 https://opentelemetry.io/ Week 11 Achievements: Kubernetes Mastery:\nSuccessfully deployed Minikube for local development Created and managed Deployments, Services, ConfigMaps Understood Pod lifecycle and container orchestration Scaled applications horizontally with ReplicaSets Production Kubernetes:\nDeployed production-ready K8s cluster on AWS using KOPS Configured high availability with multiple master nodes Deployed demo microservices application Managed cluster upgrades and node scaling Monitoring Excellence:\nInstalled Prometheus for metrics scraping Created comprehensive Grafana dashboards Configured alerting rules for critical metrics Monitored K8s cluster health and application performance Observability:\nImplemented OpenTelemetry instrumentation Collected traces, metrics, and logs in unified format Integrated Helios project for website performance monitoring Created end-to-end observability pipeline DevOps Skills:\nAutomated deployment with kubectl and Helm Understood GitOps principles Practiced infrastructure as code with K8s manifests Implemented monitoring-driven development "},{"uri":"https://khaicoderlor.github.io/intership-report-aws/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Consolidate all knowledge and skills from weeks 1-11 Complete final project with full DevOps stack Implement comprehensive CI/CD pipeline Finalize documentation and presentation Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Mon Final Project Architecture: - Design complete architecture diagram - Plan infrastructure: VPC, subnets, security groups - Define CI/CD pipeline flow Practice: Document all components and integrations 24/11/2025 24/11/2025 https://cloudjourney.awsstudygroup.com/ Tue Infrastructure Deployment: - Deploy Kubernetes cluster with KOPS - Configure ECS Fargate services - Set up databases: RDS, DynamoDB, ElastiCache Practice: Deploy containerized microservices 25/11/2025 25/11/2025 https://cloudjourney.awsstudygroup.com/ Wed CI/CD Pipeline Integration: - Configure complete Jenkins/CodePipeline - Implement Git Flow branching strategy - Set up automated testing and deployment Practice: Test automated deployment from commit to production 26/11/2025 26/11/2025 https://cloudjourney.awsstudygroup.com/ Thu Monitoring \u0026amp; Observability: - Configure Prometheus and Grafana dashboards - Set up OpenTelemetry and Helios tracing - CloudWatch integration for AWS services Practice: Create comprehensive monitoring suite 27/11/2025 27/11/2025 https://cloudjourney.awsstudygroup.com/ Fri Final Review \u0026amp; Documentation: - End-to-end testing of all components - Security review: IAM, KMS, WAF, Cognito - Complete worklog and presentation Practice: Demonstrate full system functionality 28/11/2025 28/11/2025 https://cloudjourney.awsstudygroup.com/ Week 12 Achievements: Complete Architecture:\nDesigned and deployed full-stack application on AWS Integrated 20+ AWS services seamlessly Implemented multi-AZ high availability architecture Achieved 99.9% uptime SLA DevOps Pipeline:\nBuilt end-to-end CI/CD pipeline with Jenkins and AWS CodePipeline Implemented Git Flow workflow across team Automated build, test, and deployment processes Reduced deployment time from hours to minutes Container Orchestration:\nDeployed production-ready Kubernetes cluster with KOPS Managed containerized microservices on ECS Fargate Implemented service mesh for inter-service communication Achieved auto-scaling based on traffic patterns Comprehensive Monitoring:\nDeployed Prometheus and Grafana for metrics visualization Implemented distributed tracing with OpenTelemetry and Helios Configured CloudWatch dashboards for all AWS services Set up proactive alerting with SNS notifications Security Excellence:\nImplemented defense-in-depth security strategy Encrypted all data at rest (KMS) and in transit (TLS) Configured WAF rules for application protection Achieved least privilege access with IAM roles Implemented Cognito for user authentication Technical Mastery:\nMastered 30+ AWS services across compute, storage, database, networking, security, monitoring Proficient in Docker, Kubernetes, Jenkins, Git Flow Experienced with infrastructure as code (CloudFormation, Terraform) Understood serverless architectures and event-driven patterns Final Deliverables:\nComprehensive 12-week worklog documentation Architecture diagram and technical documentation Working demo of complete application Presentation showcasing achievements and lessons learned "},{"uri":"https://khaicoderlor.github.io/intership-report-aws/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://khaicoderlor.github.io/intership-report-aws/tags/","title":"Tags","tags":[],"description":"","content":""}]